"""
åŸºäºLoRAæŠ€æœ¯çš„é—®é¢˜åˆ†ç±»å™¨è®­ç»ƒå™¨ - ä½¿ç”¨PEFTåº“å®ç°
ä½¿ç”¨BERT-base-chineseæ¨¡å‹è¿›è¡Œä½ç§©é€‚åº”å¾®è°ƒï¼Œå®ç°é—®é¢˜ä¸‰åˆ†ç±»ï¼šæ•°æ®å¹³å°ç›¸å…³ã€é€šç”¨å¯¹è¯ã€æ— å…³é—®é¢˜
ä½¿ç”¨PEFTåº“è¿›è¡Œé«˜æ•ˆå¾®è°ƒï¼Œä¿ç•™æ¨¡å‹åŸæœ‰èƒ½åŠ›
"""
import os
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    BertTokenizer, BertForSequenceClassification,
    get_linear_schedule_with_warmup
)
import numpy as np
from tqdm import tqdm
import logging
from sklearn.metrics import accuracy_score, classification_report
from torch.optim import AdamW

# PEFTåº“å¯¼å…¥
from peft import (
    LoraConfig, 
    get_peft_model, 
    TaskType,
    PeftModel
)

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QuestionDataset(Dataset):
    """é—®é¢˜åˆ†ç±»æ•°æ®é›†"""

    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        # ç¼–ç æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }


class PEFTLoRAQuestionClassifierTrainer:
    """åŸºäºPEFTåº“çš„LoRAé—®é¢˜åˆ†ç±»å™¨è®­ç»ƒå™¨"""

    def __init__(self, model_path, save_dir='E:/project/llm/lora/lora_peft_question_classifier',
                 lora_rank=8, lora_alpha=16, lora_dropout=0.1):
        self.model_path = model_path
        self.save_dir = save_dir
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)

        # ç±»åˆ«å®šä¹‰
        self.label_map = {
            0: "data_platform",  # æ•°æ®å¹³å°ç›¸å…³
            1: "general_chat",  # é€šç”¨å¯¹è¯
            2: "irrelevant"  # æ— å…³é—®é¢˜
        }

        # åˆå§‹åŒ–tokenizerå’ŒåŸºç¡€æ¨¡å‹
        self.tokenizer = BertTokenizer.from_pretrained(model_path)
        self.base_model = BertForSequenceClassification.from_pretrained(
            model_path,
            num_labels=3
        )

        # é…ç½®PEFT LoRA
        self.peft_config = LoraConfig(
            task_type=TaskType.SEQ_CLS,  # åºåˆ—åˆ†ç±»ä»»åŠ¡
            inference_mode=False,  # è®­ç»ƒæ¨¡å¼
            r=lora_rank,  # LoRAç§©
            lora_alpha=lora_alpha,  # LoRAç¼©æ”¾å‚æ•°
            lora_dropout=lora_dropout,  # LoRA dropout
            target_modules=[
                "query", "key", "value",  # attentionå±‚
                "dense"  # å‰é¦ˆç½‘ç»œå±‚
            ],
            bias="none",  # ä¸è®­ç»ƒbias
        )

        # åº”ç”¨PEFT LoRAåˆ°æ¨¡å‹
        self.model = get_peft_model(self.base_model, self.peft_config)
        self.model.to(self.device)

        # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
        self.model.print_trainable_parameters()
        
        logger.info(f"PEFT LoRAåˆ†ç±»å™¨å·²åŠ è½½åˆ°è®¾å¤‡: {self.device}")
        logger.info(f"LoRAé…ç½®: rank={lora_rank}, alpha={lora_alpha}, dropout={lora_dropout}")

    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""

        # æ•°æ®å¹³å°ç›¸å…³é—®é¢˜ï¼ˆç±»åˆ«0ï¼‰
        data_platform_questions = [
            "æ•°æ®æ¸…æ´—æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•è¿›è¡Œæ•°æ®æ¸…æ´—ï¼Ÿ",
            "æ•°æ®æ¸…æ´—æ­¥éª¤æœ‰å“ªäº›ï¼Ÿ",
            "æ•°æ®æ¸…æ´—çš„æµç¨‹",
            "æ€ä¹ˆæ¸…æ´—æ•°æ®ï¼Ÿ",
            "æ•°æ®å…¥åº“æµç¨‹",
            "å¦‚ä½•è¿›è¡Œæ•°æ®å…¥åº“ï¼Ÿ",
            "æ•°æ®å…¥åº“çš„æ­¥éª¤",
            "æ€æ ·å…¥åº“æ•°æ®ï¼Ÿ",
            "æ•°æ®å…¥åº“æ“ä½œ",
            "æ•°æ®è´¨é‡æ£€æŸ¥",
            "å¦‚ä½•æ£€æŸ¥æ•°æ®è´¨é‡ï¼Ÿ",
            "æ•°æ®è´¨é‡æ£€æŸ¥æ–¹æ³•",
            "æ•°æ®è´¨é‡å¦‚ä½•ä¿è¯ï¼Ÿ",
            "è´¨é‡æ£€æŸ¥æ­¥éª¤",
            "æ•°æ®ç›‘æ§",
            "å¦‚ä½•ç›‘æ§æ•°æ®ï¼Ÿ",
            "æ•°æ®ç›‘æ§æ–¹æ³•",
            "æ•°æ®ç›‘æ§æ€ä¹ˆåšï¼Ÿ",
            "ç›‘æ§æ•°æ®æµç¨‹",
            "æ•°æ®å®‰å…¨",
            "å¦‚ä½•ä¿è¯æ•°æ®å®‰å…¨ï¼Ÿ",
            "æ•°æ®å®‰å…¨æªæ–½",
            "æ•°æ®å®‰å…¨æ€ä¹ˆåšï¼Ÿ",
            "æ•°æ®åŠ å¯†æ–¹æ³•",
            "æ•°æ®é¢„å¤„ç†",
            "æ•°æ®è½¬æ¢",
            "æ•°æ®æ ‡å‡†åŒ–",
            "æ•°æ®å¤‡ä»½",
            "æƒé™ç®¡ç†",
            "ä»»åŠ¡è°ƒåº¦",
            "å¼‚å¸¸å¤„ç†",
            "æ•°æ®åº“è¿æ¥",
            "æ•°æ®å¤„ç†æµç¨‹",
            "ETLæµç¨‹",
            "æ•°æ®ä»“åº“",
            "æ•°æ®å¹³å°æ¶æ„",
            "æ•°æ®æ²»ç†",
            "æ•°æ®è¡€ç¼˜",
            "å…ƒæ•°æ®ç®¡ç†"
        ]

        # é€šç”¨å¯¹è¯ï¼ˆç±»åˆ«1ï¼‰
        general_chat_questions = [
            "ä½ å¥½",
            "æ‚¨å¥½",
            "hi",
            "hello",
            "æ—©ä¸Šå¥½",
            "ä¸‹åˆå¥½",
            "æ™šä¸Šå¥½",
            "è°¢è°¢",
            "è°¢è°¢ä½ ",
            "æ„Ÿè°¢",
            "ä¸å®¢æ°”",
            "å†è§",
            "æ‹œæ‹œ",
            "bye",
            "å¥½çš„",
            "çŸ¥é“äº†",
            "æ˜ç™½äº†",
            "æ”¶åˆ°",
            "OK",
            "å¯ä»¥",
            "æ²¡é—®é¢˜",
            "å¸®å¿™",
            "è¯·é—®",
            "èƒ½å¦",
            "éº»çƒ¦",
            "æ‰“æ‰°äº†",
            "ä¸å¥½æ„æ€",
            "å¯¹ä¸èµ·",
            "æŠ±æ­‰"
        ]

        # æ— å…³é—®é¢˜ï¼ˆç±»åˆ«2ï¼‰
        irrelevant_questions = [
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½åƒçš„ï¼Ÿ",
            "å¦‚ä½•å­¦ä¹ è‹±è¯­ï¼Ÿ",
            "Pythonæ€ä¹ˆå®‰è£…ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "è‚¡ç¥¨ä»Šå¤©æ¶¨äº†å—ï¼Ÿ",
            "ç”µå½±æ¨èä¸€ä¸‹",
            "éŸ³ä¹å¥½å¬å—ï¼Ÿ",
            "æ¸¸æˆæ”»ç•¥",
            "æ—…æ¸¸æ™¯ç‚¹æ¨è",
            "ç¾é£Ÿåˆ¶ä½œæ–¹æ³•",
            "å¥èº«è®¡åˆ’",
            "å‡è‚¥æ–¹æ³•",
            "æŠ¤è‚¤å“æ¨è",
            "æ±½è½¦ä¿å…»",
            "æˆ¿ä»·èµ°åŠ¿",
            "æ•™è‚²æ”¿ç­–",
            "åŒ»ç–—ä¿é™©",
            "æ³•å¾‹å’¨è¯¢",
            "å¿ƒç†å¥åº·",
            "what is your name?",
            "how are you?",
            "where are you from?",
            "tell me a joke",
            "sing a song",
            "write a poem",
            "calculate 1+1",
            "what time is it?",
            "random text here",
            "asdfghjkl",
            "123456789",
            "æµ‹è¯•æ–‡æœ¬",
            "éšæœºè¾“å…¥",
            "æ— æ„ä¹‰å†…å®¹",
            "ä¹±ä¸ƒå…«ç³Ÿ",
            "èƒ¡è¨€ä¹±è¯­",
            "ä¸çŸ¥æ‰€äº‘",
            "è«åå…¶å¦™",
            "å¥‡æ€ªé—®é¢˜",
            "æ— å…³å†…å®¹"
        ]

        # ç»„åˆæ•°æ®
        texts = data_platform_questions + general_chat_questions + irrelevant_questions
        labels = ([0] * len(data_platform_questions) +
                  [1] * len(general_chat_questions) +
                  [2] * len(irrelevant_questions))

        logger.info(f"å‡†å¤‡äº† {len(texts)} ä¸ªè®­ç»ƒæ ·æœ¬")
        logger.info(f"æ•°æ®å¹³å°ç›¸å…³: {len(data_platform_questions)} ä¸ª")
        logger.info(f"é€šç”¨å¯¹è¯: {len(general_chat_questions)} ä¸ª")
        logger.info(f"æ— å…³é—®é¢˜: {len(irrelevant_questions)} ä¸ª")

        return texts, labels

    def create_dataloader(self, texts, labels, batch_size=8, shuffle=True):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        dataset = QuestionDataset(texts, labels, self.tokenizer)
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=0  # Windowsç³»ç»Ÿå»ºè®®è®¾ä¸º0
        )
        return dataloader

    def train(self, epochs=5, batch_size=8, learning_rate=1e-4):
        """è®­ç»ƒPEFT LoRAåˆ†ç±»å™¨"""
        logger.info("å¼€å§‹è®­ç»ƒPEFT LoRAåˆ†ç±»å™¨...")

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        texts, labels = self.prepare_training_data()

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = self.create_dataloader(texts, labels, batch_size, shuffle=True)

        # è®¾ç½®ä¼˜åŒ–å™¨ - åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
        optimizer = AdamW(self.model.parameters(), lr=learning_rate, weight_decay=0.01)

        # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps = len(train_dataloader) * epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(0.1 * total_steps),  # 10%çš„æ­¥æ•°ç”¨äºwarmup
            num_training_steps=total_steps
        )

        self.model.train()
        best_loss = float('inf')

        for epoch in range(epochs):
            total_loss = 0
            progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}')

            for batch in progress_bar:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                loss = outputs.loss

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                optimizer.step()
                scheduler.step()

                total_loss += loss.item()
                progress_bar.set_postfix({'loss': loss.item()})

            avg_loss = total_loss / len(train_dataloader)
            logger.info(f'Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}')

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self.save_peft_model('best_model')

        logger.info("PEFT LoRAè®­ç»ƒå®Œæˆï¼")

    def evaluate(self, texts, labels):
        """è¯„ä¼°PEFT LoRAåˆ†ç±»å™¨"""
        logger.info("å¼€å§‹è¯„ä¼°PEFT LoRAåˆ†ç±»å™¨...")

        eval_dataloader = self.create_dataloader(texts, labels, batch_size=8, shuffle=False)

        self.model.eval()
        predictions = []
        true_labels = []

        with torch.no_grad():
            for batch in tqdm(eval_dataloader, desc='Evaluating'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)

                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                # è·å–é¢„æµ‹ç»“æœ
                logits = outputs.logits
                predicted_labels = torch.argmax(logits, dim=1)

                predictions.extend(predicted_labels.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())

        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(true_labels, predictions)
        logger.info(f"å‡†ç¡®ç‡: {accuracy:.4f}")

        # æ‰“å°åˆ†ç±»æŠ¥å‘Š
        target_names = ['æ•°æ®å¹³å°ç›¸å…³', 'é€šç”¨å¯¹è¯', 'æ— å…³é—®é¢˜']
        report = classification_report(true_labels, predictions, target_names=target_names)
        logger.info(f"åˆ†ç±»æŠ¥å‘Š:\n{report}")

        return accuracy

    def save_peft_model(self, model_name='peft_lora_question_classifier'):
        """ä¿å­˜PEFTæ¨¡å‹"""
        save_path = os.path.join(self.save_dir, model_name)
        os.makedirs(save_path, exist_ok=True)

        # ä¿å­˜PEFTæ¨¡å‹ï¼ˆåªä¿å­˜LoRAæƒé‡ï¼‰
        self.model.save_pretrained(save_path)

        # ä¿å­˜tokenizer
        self.tokenizer.save_pretrained(save_path)

        # ä¿å­˜æ ‡ç­¾æ˜ å°„
        label_map_path = os.path.join(save_path, 'label_map.json')
        with open(label_map_path, 'w', encoding='utf-8') as f:
            json.dump(self.label_map, f, indent=2, ensure_ascii=False)

        # ä¿å­˜é…ç½®ä¿¡æ¯
        config_path = os.path.join(save_path, 'training_config.json')
        config = {
            'base_model_path': self.model_path,
            'lora_rank': self.lora_rank,
            'lora_alpha': self.lora_alpha,
            'lora_dropout': self.lora_dropout,
            'num_labels': 3,
            'label_map': self.label_map
        }
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)

        logger.info(f"PEFTæ¨¡å‹å·²ä¿å­˜åˆ°: {save_path}")
        return save_path

    def predict(self, text):
        """ä½¿ç”¨PEFT LoRAæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        self.model.eval()

        # ç¼–ç è¾“å…¥æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=128,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)

        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=1)
            predicted_class = torch.argmax(logits, dim=1).item()
            confidence = probabilities[0][predicted_class].item()

        return {
            'category': self.label_map[predicted_class],
            'confidence': confidence,
            'probabilities': probabilities[0].cpu().numpy().tolist()
        }


class PEFTLoRAQuestionClassifier:
    """PEFT LoRAé—®é¢˜åˆ†ç±»å™¨æ¨ç†ç±»"""

    def __init__(self, base_model_path, peft_model_path):
        self.base_model_path = base_model_path
        self.peft_model_path = peft_model_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åŠ è½½é…ç½®
        config_path = os.path.join(peft_model_path, 'training_config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)

        # åŠ è½½æ ‡ç­¾æ˜ å°„
        label_map_path = os.path.join(peft_model_path, 'label_map.json')
        with open(label_map_path, 'r', encoding='utf-8') as f:
            self.label_map = json.load(f)
            # è½¬æ¢é”®ä¸ºæ•´æ•°
            self.label_map = {int(k): v for k, v in self.label_map.items()}

        # åˆå§‹åŒ–tokenizer
        self.tokenizer = BertTokenizer.from_pretrained(peft_model_path)

        # åŠ è½½åŸºç¡€æ¨¡å‹
        self.base_model = BertForSequenceClassification.from_pretrained(
            base_model_path,
            num_labels=self.config['num_labels']
        )

        # åŠ è½½PEFTæ¨¡å‹
        self.model = PeftModel.from_pretrained(self.base_model, peft_model_path)
        self.model.to(self.device)
        self.model.eval()

        logger.info(f"PEFT LoRAåˆ†ç±»å™¨æ¨ç†æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.device}")

    def classify_question(self, text):
        """å¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»"""
        # ç¼–ç è¾“å…¥æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=128,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)

        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=1)
            predicted_class = torch.argmax(logits, dim=1).item()
            confidence = probabilities[0][predicted_class].item()

        return {
            'category': self.label_map[predicted_class],
            'confidence': confidence,
            'probabilities': probabilities[0].cpu().numpy().tolist(),
            'method': 'peft_lora_bert'
        }


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # æ¨¡å‹è·¯å¾„ - ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„
    bert_model_path = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea"

    print("=" * 60)
    print("ğŸš€ PEFT LoRAé—®é¢˜åˆ†ç±»å™¨è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    print("åŸºäºBERT-base-chineseæ¨¡å‹ï¼Œä½¿ç”¨PEFTåº“è¿›è¡ŒLoRAé«˜æ•ˆå¾®è°ƒ")
    print("ä¿ç•™æ¨¡å‹åŸæœ‰èƒ½åŠ›çš„åŒæ—¶å®ç°é—®é¢˜ä¸‰åˆ†ç±»")
    print("=" * 60)

    # åˆ›å»ºPEFT LoRAè®­ç»ƒå™¨
    trainer = PEFTLoRAQuestionClassifierTrainer(
        model_path=bert_model_path,
        save_dir='E:/project/llm/lora/peft_lora_question_classifier',
        lora_rank=8,  # LoRAç§©ï¼Œæ§åˆ¶å‚æ•°é‡
        lora_alpha=16,  # LoRAç¼©æ”¾å› å­
        lora_dropout=0.1  # LoRA dropoutç‡
    )

    # å¼€å§‹è®­ç»ƒ
    print("\nå¼€å§‹PEFT LoRAè®­ç»ƒ...")
    trainer.train(
        epochs=5,  # è®­ç»ƒè½®æ•°
        batch_size=8,  # æ‰¹æ¬¡å¤§å°
        learning_rate=1e-4  # å­¦ä¹ ç‡
    )

    # è¯„ä¼°æ¨¡å‹
    print("\nå¼€å§‹æ¨¡å‹è¯„ä¼°...")
    texts, labels = trainer.prepare_training_data()
    accuracy = trainer.evaluate(texts, labels)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    save_path = trainer.save_peft_model('final_model')

    print("\n" + "=" * 60)
    print("âœ… PEFT LoRAè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {save_path}")
    print("=" * 60)

    # æµ‹è¯•æ¨ç†
    print("\nğŸ§ª æµ‹è¯•PEFT LoRAæ¨ç†...")
    try:
        # åˆ›å»ºæ¨ç†å™¨
        classifier = PEFTLoRAQuestionClassifier(
            base_model_path=bert_model_path,
            peft_model_path=save_path
        )

        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "æ•°æ®æ¸…æ´—æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",  # æ•°æ®å¹³å°ç›¸å…³
            "ä½ å¥½",  # é€šç”¨å¯¹è¯
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",  # æ— å…³é—®é¢˜
        ]

        print("\næµ‹è¯•ç»“æœï¼š")
        print("-" * 40)
        for question in test_questions:
            result = classifier.classify_question(question)
            category_cn = {
                "data_platform": "æ•°æ®å¹³å°ç›¸å…³",
                "general_chat": "é€šç”¨å¯¹è¯",
                "irrelevant": "æ— å…³é—®é¢˜"
            }
            print(f"é—®é¢˜: {question}")
            print(f"åˆ†ç±»: {category_cn[result['category']]} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
            print("-" * 40)

    except Exception as e:
        print(f"æ¨ç†æµ‹è¯•å¤±è´¥: {e}")

    return save_path


def test_peft_lora_classifier():
    """æµ‹è¯•å·²è®­ç»ƒçš„PEFT LoRAåˆ†ç±»å™¨"""
    bert_model_path = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea"
    peft_model_path = r"E:\project\llm\lora\peft_lora_question_classifier\final_model"

    if not os.path.exists(peft_model_path):
        print(f"PEFT LoRAæ¨¡å‹ä¸å­˜åœ¨: {peft_model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒç¨‹åº")
        return

    print("=" * 60)
    print("ğŸ§ª PEFT LoRAé—®é¢˜åˆ†ç±»å™¨æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = PEFTLoRAQuestionClassifier(
        base_model_path=bert_model_path,
        peft_model_path=peft_model_path
    )

    # æµ‹è¯•é—®é¢˜
    test_questions = [
        # æ•°æ®å¹³å°ç›¸å…³
        "æ•°æ®æ¸…æ´—æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•è¿›è¡Œæ•°æ®å…¥åº“ï¼Ÿ",
        "æ•°æ®è´¨é‡æ£€æŸ¥æ–¹æ³•",
        "æ•°æ®ç›‘æ§æ€ä¹ˆåšï¼Ÿ",
        "æ•°æ®å®‰å…¨æªæ–½",

        # é€šç”¨å¯¹è¯
        "ä½ å¥½",
        "è°¢è°¢",
        "å†è§",
        "ä¸å®¢æ°”",
        "å¥½çš„",

        # æ— å…³é—®é¢˜
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½åƒçš„ï¼Ÿ",
        "å¦‚ä½•å­¦ä¹ è‹±è¯­ï¼Ÿ",
        "what is your name?",
        "asdfghjkl"
    ]

    print("æµ‹è¯•ç»“æœï¼š")
    print("-" * 60)

    category_cn = {
        "data_platform": "æ•°æ®å¹³å°ç›¸å…³",
        "general_chat": "é€šç”¨å¯¹è¯",
        "irrelevant": "æ— å…³é—®é¢˜"
    }

    for question in test_questions:
        result = classifier.classify_question(question)
        print(f"é—®é¢˜: {question}")
        print(f"åˆ†ç±»: {category_cn[result['category']]} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
        print(f"æ–¹æ³•: {result['method']}")
        print("-" * 40)


def interactive_peft_lora_test():
    """äº¤äº’å¼PEFT LoRAåˆ†ç±»å™¨æµ‹è¯•"""
    bert_model_path = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea"
    peft_model_path = r"E:\project\llm\lora\peft_lora_question_classifier\final_model"

    if not os.path.exists(peft_model_path):
        print(f"PEFT LoRAæ¨¡å‹ä¸å­˜åœ¨: {peft_model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒç¨‹åº")
        return

    print("=" * 60)
    print("ğŸ¤– äº¤äº’å¼PEFT LoRAåˆ†ç±»å™¨æµ‹è¯•")
    print("=" * 60)

    classifier = PEFTLoRAQuestionClassifier(
        base_model_path=bert_model_path,
        peft_model_path=peft_model_path
    )

    category_cn = {
        "data_platform": "æ•°æ®å¹³å°ç›¸å…³",
        "general_chat": "é€šç”¨å¯¹è¯",
        "irrelevant": "æ— å…³é—®é¢˜"
    }

    print("è¾“å…¥é—®é¢˜è¿›è¡Œåˆ†ç±»æµ‹è¯•ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("-" * 60)

    while True:
        try:
            question = input("è¯·è¾“å…¥é—®é¢˜: ").strip()

            if question.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                break

            if not question:
                continue

            result = classifier.classify_question(question)

            print(f"åˆ†ç±»ç»“æœ: {category_cn[result['category']]}")
            print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
            print(f"åˆ†ç±»æ–¹æ³•: {result['method']}")
            print("-" * 40)

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"å¤„ç†é”™è¯¯: {e}")


if __name__ == "__main__":
    # é»˜è®¤æ˜¾ç¤ºèœå•
    while True:
        print("\n" + "=" * 60)
        print("ğŸ¤– PEFT LoRAé—®é¢˜åˆ†ç±»å™¨ç³»ç»Ÿ")
        print("=" * 60)
        print("1. è®­ç»ƒPEFT LoRAåˆ†ç±»å™¨")
        print("2. æµ‹è¯•PEFT LoRAåˆ†ç±»å™¨")
        print("3. äº¤äº’å¼æµ‹è¯•")
        print("0. é€€å‡º")
        print("=" * 60)

        try:
            choice = input("è¯·é€‰æ‹©åŠŸèƒ½ (0-3): ").strip()

            if choice == "1":
                main()
            elif choice == "2":
                test_peft_lora_classifier()
            elif choice == "3":
                interactive_peft_lora_test()
            elif choice == "0":
                print("æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼")
                break
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

        except KeyboardInterrupt:
            print("\næ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼")
            break
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {e}")
