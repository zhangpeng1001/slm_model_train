"""
åŸºäºPEFTåº“çš„BERT-base-chineseå¤šæ¨¡å¼æŒ‡ä»¤å¾®è°ƒè®­ç»ƒå™¨
æ”¯æŒ4ç§æ•°æ®é›†çš„ä¸åŒè®­ç»ƒæ¨¡å¼ï¼Œéƒ½æ”¯æŒæŒ‡ä»¤å¾®è°ƒ
"""

import os
import json
import torch
import random
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import (
    BertTokenizer,
    BertModel,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType
)
import logging
from tqdm import tqdm
from dataset import question_classifier, question_type_classifier, question_answer, file_name_pick
import torch.nn as nn
from typing import Dict, List, Optional, Union
import argparse

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)


set_seed(42)


class BertForInstructionTuning(nn.Module):
    """è‡ªå®šä¹‰BERTæ¨¡å‹ï¼Œæ”¯æŒæŒ‡ä»¤å¾®è°ƒ"""

    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model
        self.config = bert_model.config
        # æ·»åŠ è¯­è¨€å»ºæ¨¡å¤´
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)

    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):
        # è¿‡æ»¤æ‰BERTä¸æ”¯æŒçš„å‚æ•°
        bert_kwargs = {}
        for key, value in kwargs.items():
            if key not in ['num_items_in_batch']:  # è¿‡æ»¤æ‰è®­ç»ƒå™¨ä¼ é€’çš„é¢å¤–å‚æ•°
                bert_kwargs[key] = value

        # è·å–BERTè¾“å‡º
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **bert_kwargs
        )

        # è®¡ç®—logits
        hidden_states = outputs.last_hidden_state
        logits = self.lm_head(hidden_states)

        # å¦‚æœæœ‰labelsï¼Œè®¡ç®—æŸå¤±ï¼ˆç”¨äºè®­ç»ƒï¼‰
        if labels is not None:
            # è®¡ç®—æŸå¤±
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()

            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

            # åˆ›å»ºä¸€ä¸ªå¯ä»¥è¢«ç´¢å¼•çš„è¾“å‡ºå¯¹è±¡
            class ModelOutput:
                def __init__(self, loss, logits, last_hidden_state):
                    self.loss = loss
                    self.logits = logits
                    self.last_hidden_state = last_hidden_state

                def __getitem__(self, key):
                    if key == 0:
                        return self.loss
                    elif key == 1:
                        return self.logits
                    else:
                        raise IndexError(f"Index {key} out of range")

                def __contains__(self, key):
                    return key in ['loss', 'logits', 'last_hidden_state']

                def __getattr__(self, name):
                    if name == 'loss':
                        return self.loss
                    elif name == 'logits':
                        return self.logits
                    elif name == 'last_hidden_state':
                        return self.last_hidden_state
                    else:
                        raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")

            return ModelOutput(loss, logits, hidden_states)

        # åˆ›å»ºä¸€ä¸ªå¯ä»¥è¢«ç´¢å¼•çš„è¾“å‡ºå¯¹è±¡ï¼ˆæ— æŸå¤±ï¼‰
        class ModelOutput:
            def __init__(self, logits, last_hidden_state):
                self.logits = logits
                self.last_hidden_state = last_hidden_state

            def __getitem__(self, key):
                if key == 0:
                    return self.logits
                else:
                    raise IndexError(f"Index {key} out of range")

            def __contains__(self, key):
                return key in ['logits', 'last_hidden_state']

        return ModelOutput(logits, hidden_states)

    def prepare_inputs_for_generation(self, input_ids, **kwargs):
        """ä¸ºç”Ÿæˆå‡†å¤‡è¾“å…¥ï¼ŒPEFTéœ€è¦è¿™ä¸ªæ–¹æ³•"""
        return {"input_ids": input_ids}

    def get_input_embeddings(self):
        """è·å–è¾“å…¥åµŒå…¥å±‚"""
        return self.bert.embeddings.word_embeddings

    def set_input_embeddings(self, value):
        """è®¾ç½®è¾“å…¥åµŒå…¥å±‚"""
        self.bert.embeddings.word_embeddings = value

    def get_output_embeddings(self):
        """è·å–è¾“å‡ºåµŒå…¥å±‚"""
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        """è®¾ç½®è¾“å‡ºåµŒå…¥å±‚"""
        self.lm_head = new_embeddings


class InstructionDataset(Dataset):
    """æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†"""

    def __init__(self, data, tokenizer, max_length=512, dataset_type="mixed"):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.dataset_type = dataset_type

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # æ„å»ºæŒ‡ä»¤æ ¼å¼çš„å®Œæ•´æ–‡æœ¬ï¼ˆè¾“å…¥+è¾“å‡ºï¼‰
        instruction = item["instruction"]
        input_text = item["input"]
        output_text = item["output"]

        # æ ¹æ®æ•°æ®é›†ç±»å‹æ„å»ºä¸åŒçš„æŒ‡ä»¤æ ¼å¼
        if self.dataset_type == "question_classifier":
            full_text = f"ä»»åŠ¡ï¼šé—®é¢˜åœºæ™¯åˆ†ç±»\næŒ‡ä»¤ï¼š{instruction}\né—®é¢˜ï¼š{input_text}\nåˆ†ç±»ç»“æœï¼š{output_text}"
        elif self.dataset_type == "question_type_classifier":
            full_text = f"ä»»åŠ¡ï¼šé—®é¢˜ç±»å‹åˆ†ç±»\næŒ‡ä»¤ï¼š{instruction}\né—®é¢˜ï¼š{input_text}\nç±»å‹ï¼š{output_text}"
        elif self.dataset_type == "question_answer":
            full_text = f"ä»»åŠ¡ï¼šé—®é¢˜å›ç­”\næŒ‡ä»¤ï¼š{instruction}\né—®é¢˜ï¼š{input_text}\nå›ç­”ï¼š{output_text}"
        elif self.dataset_type == "file_name_pick":
            full_text = f"ä»»åŠ¡ï¼šæ–‡ä»¶åç§°æå–\næŒ‡ä»¤ï¼š{instruction}\næ–‡æœ¬ï¼š{input_text}\næå–ç»“æœï¼š{output_text}"
        else:  # mixed mode
            full_text = f"æŒ‡ä»¤ï¼š{instruction}\nè¾“å…¥ï¼š{input_text}\nè¾“å‡ºï¼š{output_text}"

        # ç¼–ç æ–‡æœ¬
        encoding = self.tokenizer(
            full_text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()  # å¯¹äºè¯­è¨€å»ºæ¨¡ï¼Œæ ‡ç­¾å°±æ˜¯input_ids
        }


class DatasetManager:
    """æ•°æ®é›†ç®¡ç†å™¨ï¼Œæ”¯æŒä¸åŒçš„è®­ç»ƒæ¨¡å¼"""
    
    def __init__(self):
        self.datasets = {
            "question_classifier": question_classifier,
            "question_type_classifier": question_type_classifier,
            "question_answer": question_answer,
            "file_name_pick": file_name_pick
        }
    
    def get_dataset(self, dataset_type: str) -> List[Dict]:
        """è·å–æŒ‡å®šç±»å‹çš„æ•°æ®é›†"""
        if dataset_type == "all" or dataset_type == "mixed":
            # åˆå¹¶æ‰€æœ‰æ•°æ®é›†
            all_data = []
            for data in self.datasets.values():
                all_data.extend(data)
            random.shuffle(all_data)
            return all_data
        elif dataset_type in self.datasets:
            data = self.datasets[dataset_type].copy()
            random.shuffle(data)
            return data
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {dataset_type}")
    
    def get_dataset_info(self, dataset_type: str) -> Dict:
        """è·å–æ•°æ®é›†ä¿¡æ¯"""
        if dataset_type == "all" or dataset_type == "mixed":
            total_samples = sum(len(data) for data in self.datasets.values())
            info = {
                "total_samples": total_samples,
                "datasets": {name: len(data) for name, data in self.datasets.items()}
            }
        elif dataset_type in self.datasets:
            info = {
                "total_samples": len(self.datasets[dataset_type]),
                "dataset_type": dataset_type
            }
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {dataset_type}")
        
        return info


class MultiModeTrainer:
    """å¤šæ¨¡å¼è®­ç»ƒå™¨"""
    
    def __init__(self, 
                 bert_model_path: str = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea",
                 base_output_dir: str = r"E:\project\llm\lora"):
        self.bert_model_path = bert_model_path
        self.base_output_dir = base_output_dir
        self.dataset_manager = DatasetManager()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # è®­ç»ƒé…ç½®
        self.training_configs = {
            "question_classifier": {
                "epochs": 5,
                "batch_size": 8,
                "learning_rate": 2e-4,
                "lora_r": 16,
                "lora_alpha": 32,
                "description": "é—®é¢˜åœºæ™¯åˆ†ç±»ä»»åŠ¡"
            },
            "question_type_classifier": {
                "epochs": 4,
                "batch_size": 8,
                "learning_rate": 1.5e-4,
                "lora_r": 12,
                "lora_alpha": 24,
                "description": "é—®é¢˜ç±»å‹åˆ†ç±»ä»»åŠ¡"
            },
            "question_answer": {
                "epochs": 6,
                "batch_size": 6,
                "learning_rate": 2.5e-4,
                "lora_r": 20,
                "lora_alpha": 40,
                "description": "é—®é¢˜å›ç­”ä»»åŠ¡"
            },
            "file_name_pick": {
                "epochs": 4,
                "batch_size": 8,
                "learning_rate": 2e-4,
                "lora_r": 16,
                "lora_alpha": 32,
                "description": "æ–‡ä»¶åç§°æå–ä»»åŠ¡"
            },
            "mixed": {
                "epochs": 5,
                "batch_size": 8,
                "learning_rate": 2e-4,
                "lora_r": 16,
                "lora_alpha": 32,
                "description": "æ··åˆå¤šä»»åŠ¡è®­ç»ƒ"
            }
        }
    
    def prepare_model_and_tokenizer(self, config: Dict):
        """å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨"""
        logger.info(f"ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # åŠ è½½tokenizerå’Œæ¨¡å‹
        tokenizer = BertTokenizer.from_pretrained(self.bert_model_path)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.unk_token
        
        # åŠ è½½åŸºç¡€BERTæ¨¡å‹
        bert_model = BertModel.from_pretrained(self.bert_model_path)
        
        # ä½¿ç”¨è‡ªå®šä¹‰åŒ…è£…å™¨
        model = BertForInstructionTuning(bert_model)
        
        # é…ç½®LoRA
        lora_config = LoraConfig(
            r=config["lora_r"],
            lora_alpha=config["lora_alpha"],
            target_modules=["query", "key", "value", "dense"],
            lora_dropout=0.1,
            bias="none",
            task_type=TaskType.CAUSAL_LM,
        )
        
        # åº”ç”¨LoRA
        model = get_peft_model(model, lora_config)
        model.to(self.device)
        
        # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
        model.print_trainable_parameters()
        
        return model, tokenizer, lora_config
    
    def train_single_dataset(self, dataset_type: str, custom_output_dir: Optional[str] = None):
        """è®­ç»ƒå•ä¸ªæ•°æ®é›†"""
        if dataset_type not in self.training_configs:
            raise ValueError(f"ä¸æ”¯æŒçš„æ•°æ®é›†ç±»å‹: {dataset_type}")
        
        config = self.training_configs[dataset_type]
        logger.info(f"å¼€å§‹è®­ç»ƒ {config['description']} ({dataset_type})")
        
        # å‡†å¤‡è¾“å‡ºç›®å½•
        if custom_output_dir:
            output_dir = custom_output_dir
        else:
            output_dir = os.path.join(self.base_output_dir, f"peft_lora_{dataset_type}")
        os.makedirs(output_dir, exist_ok=True)
        
        # å‡†å¤‡æ¨¡å‹å’Œåˆ†è¯å™¨
        model, tokenizer, lora_config = self.prepare_model_and_tokenizer(config)
        
        # å‡†å¤‡æ•°æ®
        logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        train_data = self.dataset_manager.get_dataset(dataset_type)
        dataset_info = self.dataset_manager.get_dataset_info(dataset_type)
        
        logger.info(f"æ•°æ®é›†ä¿¡æ¯: {dataset_info}")
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = InstructionDataset(train_data, tokenizer, max_length=512, dataset_type=dataset_type)
        
        # æ•°æ®æ”¶é›†å™¨
        data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
        
        # è®­ç»ƒå‚æ•°
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=config["epochs"],
            per_device_train_batch_size=config["batch_size"],
            per_device_eval_batch_size=config["batch_size"],
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir=os.path.join(output_dir, 'logs'),
            logging_steps=50,
            save_steps=500,
            save_strategy="steps",
            learning_rate=config["learning_rate"],
            fp16=torch.cuda.is_available(),
            dataloader_pin_memory=False,
            remove_unused_columns=False,
            report_to=None,
            load_best_model_at_end=False,
        )
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            tokenizer=tokenizer,
            data_collator=data_collator,
        )
        
        logger.info("å¼€å§‹è®­ç»ƒ...")
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        logger.info("ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        
        # ä¿å­˜é…ç½®ä¿¡æ¯
        training_config = {
            "bert_model_path": self.bert_model_path,
            "dataset_type": dataset_type,
            "dataset_info": dataset_info,
            "lora_config": {
                "r": config["lora_r"],
                "lora_alpha": config["lora_alpha"],
                "lora_dropout": 0.1,
                "target_modules": ["query", "key", "value", "dense"]
            },
            "training_info": {
                "total_samples": dataset_info["total_samples"],
                "epochs": config["epochs"],
                "batch_size": config["batch_size"],
                "learning_rate": config["learning_rate"],
                "description": config["description"]
            }
        }
        
        config_path = os.path.join(output_dir, "training_config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(training_config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… {config['description']} è®­ç»ƒå®Œæˆï¼")
        logger.info(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {output_dir}")
        logger.info(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ•°: {dataset_info['total_samples']}")
        
        return output_dir
    
    def train_all_datasets_separately(self):
        """åˆ†åˆ«è®­ç»ƒæ‰€æœ‰æ•°æ®é›†"""
        results = {}
        
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹åˆ†åˆ«è®­ç»ƒæ‰€æœ‰æ•°æ®é›†")
        logger.info("=" * 60)
        
        for dataset_type in ["question_classifier", "question_type_classifier", "question_answer", "file_name_pick"]:
            try:
                logger.info(f"\n{'='*20} {dataset_type} {'='*20}")
                output_dir = self.train_single_dataset(dataset_type)
                results[dataset_type] = {
                    "status": "success",
                    "output_dir": output_dir
                }
            except Exception as e:
                logger.error(f"âŒ {dataset_type} è®­ç»ƒå¤±è´¥: {e}")
                results[dataset_type] = {
                    "status": "failed",
                    "error": str(e)
                }
        
        return results
    
    def train_mixed_dataset(self):
        """è®­ç»ƒæ··åˆæ•°æ®é›†"""
        logger.info("=" * 60)
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒæ··åˆæ•°æ®é›†")
        logger.info("=" * 60)
        
        return self.train_single_dataset("mixed")
    
    def get_available_datasets(self):
        """è·å–å¯ç”¨çš„æ•°æ®é›†ç±»å‹"""
        return list(self.training_configs.keys())
    
    def print_dataset_summary(self):
        """æ‰“å°æ•°æ®é›†æ‘˜è¦"""
        logger.info("=" * 60)
        logger.info("ğŸ“Š æ•°æ®é›†æ‘˜è¦")
        logger.info("=" * 60)
        
        for dataset_type in self.dataset_manager.datasets.keys():
            info = self.dataset_manager.get_dataset_info(dataset_type)
            config = self.training_configs[dataset_type]
            logger.info(f"{config['description']}: {info['total_samples']} ä¸ªæ ·æœ¬")
        
        # æ··åˆæ•°æ®é›†ä¿¡æ¯
        mixed_info = self.dataset_manager.get_dataset_info("mixed")
        logger.info(f"æ··åˆæ•°æ®é›†: {mixed_info['total_samples']} ä¸ªæ ·æœ¬")
        logger.info(f"è¯¦ç»†åˆ†å¸ƒ: {mixed_info['datasets']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description="PEFT LoRA å¤šæ¨¡å¼æŒ‡ä»¤å¾®è°ƒè®­ç»ƒå™¨")
    parser.add_argument(
        "--mode", 
        type=str, 
        choices=["single", "all", "mixed", "summary"], 
        default="mixed",
        help="è®­ç»ƒæ¨¡å¼: single(å•ä¸ªæ•°æ®é›†), all(æ‰€æœ‰æ•°æ®é›†åˆ†åˆ«è®­ç»ƒ), mixed(æ··åˆè®­ç»ƒ), summary(æ˜¾ç¤ºæ•°æ®é›†æ‘˜è¦)"
    )
    parser.add_argument(
        "--dataset", 
        type=str, 
        choices=["question_classifier", "question_type_classifier", "question_answer", "file_name_pick"],
        help="å½“modeä¸ºsingleæ—¶ï¼ŒæŒ‡å®šè¦è®­ç»ƒçš„æ•°æ®é›†ç±»å‹"
    )
    parser.add_argument(
        "--output_dir", 
        type=str, 
        help="è‡ªå®šä¹‰è¾“å‡ºç›®å½•"
    )
    parser.add_argument(
        "--bert_model_path", 
        type=str, 
        default=r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea",
        help="BERTæ¨¡å‹è·¯å¾„"
    )
    parser.add_argument(
        "--base_output_dir", 
        type=str, 
        default=r"E:\project\llm\lora",
        help="åŸºç¡€è¾“å‡ºç›®å½•"
    )
    
    args = parser.parse_args()
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = MultiModeTrainer(
        bert_model_path=args.bert_model_path,
        base_output_dir=args.base_output_dir
    )
    
    print("=" * 60)
    print("ğŸš€ PEFT LoRA å¤šæ¨¡å¼æŒ‡ä»¤å¾®è°ƒè®­ç»ƒç³»ç»Ÿ")
    print("åŸºäºBERT-base-chineseçš„å¤šä»»åŠ¡æŒ‡ä»¤å¾®è°ƒ")
    print("=" * 60)
    
    try:
        if args.mode == "summary":
            # æ˜¾ç¤ºæ•°æ®é›†æ‘˜è¦
            trainer.print_dataset_summary()
            
        elif args.mode == "single":
            # è®­ç»ƒå•ä¸ªæ•°æ®é›†
            if not args.dataset:
                print("âŒ é”™è¯¯: singleæ¨¡å¼éœ€è¦æŒ‡å®š--datasetå‚æ•°")
                print(f"å¯ç”¨çš„æ•°æ®é›†ç±»å‹: {trainer.get_available_datasets()}")
                return
            
            print(f"ğŸ¯ å¼€å§‹è®­ç»ƒå•ä¸ªæ•°æ®é›†: {args.dataset}")
            output_dir = trainer.train_single_dataset(args.dataset, args.output_dir)
            print(f"\nâœ… å•ä¸ªæ•°æ®é›†è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {output_dir}")
            
        elif args.mode == "all":
            # åˆ†åˆ«è®­ç»ƒæ‰€æœ‰æ•°æ®é›†
            print("ğŸ¯ å¼€å§‹åˆ†åˆ«è®­ç»ƒæ‰€æœ‰æ•°æ®é›†")
            results = trainer.train_all_datasets_separately()
            
            print("\n" + "=" * 60)
            print("ğŸ“Š è®­ç»ƒç»“æœæ±‡æ€»")
            print("=" * 60)
            
            success_count = 0
            for dataset_type, result in results.items():
                if result["status"] == "success":
                    print(f"âœ… {dataset_type}: æˆåŠŸ - {result['output_dir']}")
                    success_count += 1
                else:
                    print(f"âŒ {dataset_type}: å¤±è´¥ - {result['error']}")
            
            print(f"\nğŸ‰ è®­ç»ƒå®Œæˆï¼æˆåŠŸ: {success_count}/{len(results)}")
            
        elif args.mode == "mixed":
            # è®­ç»ƒæ··åˆæ•°æ®é›†
            print("ğŸ¯ å¼€å§‹è®­ç»ƒæ··åˆæ•°æ®é›†")
            output_dir = trainer.train_mixed_dataset()
            print(f"\nâœ… æ··åˆæ•°æ®é›†è®­ç»ƒå®Œæˆï¼")
            print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {output_dir}")
        
        print("\n" + "=" * 60)
        print("ğŸ‰ æ‰€æœ‰è®­ç»ƒä»»åŠ¡å®Œæˆï¼")
        print("ğŸ’¡ æç¤ºï¼šå¯ä»¥ä½¿ç”¨æ¨ç†è„šæœ¬æµ‹è¯•è®­ç»ƒå¥½çš„æ¨¡å‹")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


# å…¼å®¹æ€§å‡½æ•°ï¼Œä¿æŒä¸åŸç‰ˆæœ¬çš„å…¼å®¹æ€§
def prepare_all_data():
    """å‡†å¤‡æ‰€æœ‰æ•°æ®é›†å¹¶æ‰“ä¹±ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰"""
    manager = DatasetManager()
    return manager.get_dataset("mixed")


def train_model():
    """è®­ç»ƒæ¨¡å‹ï¼ˆå…¼å®¹æ€§å‡½æ•°ï¼‰"""
    trainer = MultiModeTrainer()
    return trainer.train_mixed_dataset()


if __name__ == "__main__":
    # æ£€æŸ¥æ˜¯å¦æœ‰å‘½ä»¤è¡Œå‚æ•°
    import sys
    if len(sys.argv) == 1:
        # æ²¡æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨é»˜è®¤çš„æ··åˆè®­ç»ƒæ¨¡å¼
        print("=" * 60)
        print("ğŸš€ PEFT LoRA æ•°æ®å¹³å°è®­ç»ƒç³»ç»Ÿ")
        print("åŸºäºBERT-base-chineseçš„æŒ‡ä»¤å¾®è°ƒ")
        print("=" * 60)
        
        try:
            trainer = MultiModeTrainer()
            trainer.print_dataset_summary()
            
            print("\nğŸ¯ å¼€å§‹æ··åˆæ•°æ®é›†è®­ç»ƒ...")
            model_path = trainer.train_mixed_dataset()
            
            print("\n" + "=" * 60)
            print("âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼")
            print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
            print("ğŸ‰ å¯ä»¥å¼€å§‹ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹äº†ï¼")
            print("=" * 60)
            
        except Exception as e:
            print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    else:
        # æœ‰å‘½ä»¤è¡Œå‚æ•°ï¼Œä½¿ç”¨å‚æ•°è§£æ
        main()
