"""
---å¤±è´¥---
åŸºäºPEFTåº“çš„BERT-base-chineseæŒ‡ä»¤å¾®è°ƒè®­ç»ƒå™¨
ä¸€æ¬¡æ€§è®­ç»ƒï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®é›†ï¼Œç”Ÿæˆå•ä¸ªLoRAæ¨¡å‹
"""

import os
import json
import torch
import random
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import (
    BertTokenizer,
    BertModel,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType
)
import logging
from tqdm import tqdm
from dataset import question_classifier, question_type_classifier, question_answer, file_name_pick
import torch.nn as nn

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)


set_seed(42)


class BertForInstructionTuning(nn.Module):
    """è‡ªå®šä¹‰BERTæ¨¡å‹ï¼Œæ”¯æŒæŒ‡ä»¤å¾®è°ƒ"""

    def __init__(self, bert_model):
        super().__init__()
        self.bert = bert_model
        self.config = bert_model.config
        # æ·»åŠ è¯­è¨€å»ºæ¨¡å¤´
        self.lm_head = nn.Linear(self.config.hidden_size, self.config.vocab_size, bias=False)

    def forward(self, input_ids, attention_mask=None, labels=None, **kwargs):
        # è¿‡æ»¤æ‰BERTä¸æ”¯æŒçš„å‚æ•°
        bert_kwargs = {}
        for key, value in kwargs.items():
            if key not in ['num_items_in_batch']:  # è¿‡æ»¤æ‰è®­ç»ƒå™¨ä¼ é€’çš„é¢å¤–å‚æ•°
                bert_kwargs[key] = value

        # è·å–BERTè¾“å‡º
        outputs = self.bert(
            input_ids=input_ids,
            attention_mask=attention_mask,
            **bert_kwargs
        )

        # è®¡ç®—logits
        hidden_states = outputs.last_hidden_state
        logits = self.lm_head(hidden_states)

        # å¦‚æœæœ‰labelsï¼Œè®¡ç®—æŸå¤±ï¼ˆç”¨äºè®­ç»ƒï¼‰
        if labels is not None:
            # è®¡ç®—æŸå¤±
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()

            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))

            # åˆ›å»ºä¸€ä¸ªå¯ä»¥è¢«ç´¢å¼•çš„è¾“å‡ºå¯¹è±¡
            class ModelOutput:
                def __init__(self, loss, logits, last_hidden_state):
                    self.loss = loss
                    self.logits = logits
                    self.last_hidden_state = last_hidden_state

                def __getitem__(self, key):
                    if key == 0:
                        return self.loss
                    elif key == 1:
                        return self.logits
                    else:
                        raise IndexError(f"Index {key} out of range")

                def __contains__(self, key):
                    return key in ['loss', 'logits', 'last_hidden_state']

                def __getattr__(self, name):
                    if name == 'loss':
                        return self.loss
                    elif name == 'logits':
                        return self.logits
                    elif name == 'last_hidden_state':
                        return self.last_hidden_state
                    else:
                        raise AttributeError(f"'{type(self).__name__}' object has no attribute '{name}'")

            return ModelOutput(loss, logits, hidden_states)

        # åˆ›å»ºä¸€ä¸ªå¯ä»¥è¢«ç´¢å¼•çš„è¾“å‡ºå¯¹è±¡ï¼ˆæ— æŸå¤±ï¼‰
        class ModelOutput:
            def __init__(self, logits, last_hidden_state):
                self.logits = logits
                self.last_hidden_state = last_hidden_state

            def __getitem__(self, key):
                if key == 0:
                    return self.logits
                else:
                    raise IndexError(f"Index {key} out of range")

            def __contains__(self, key):
                return key in ['logits', 'last_hidden_state']

        return ModelOutput(logits, hidden_states)

    def prepare_inputs_for_generation(self, input_ids, **kwargs):
        """ä¸ºç”Ÿæˆå‡†å¤‡è¾“å…¥ï¼ŒPEFTéœ€è¦è¿™ä¸ªæ–¹æ³•"""
        return {"input_ids": input_ids}

    def get_input_embeddings(self):
        """è·å–è¾“å…¥åµŒå…¥å±‚"""
        return self.bert.embeddings.word_embeddings

    def set_input_embeddings(self, value):
        """è®¾ç½®è¾“å…¥åµŒå…¥å±‚"""
        self.bert.embeddings.word_embeddings = value

    def get_output_embeddings(self):
        """è·å–è¾“å‡ºåµŒå…¥å±‚"""
        return self.lm_head

    def set_output_embeddings(self, new_embeddings):
        """è®¾ç½®è¾“å‡ºåµŒå…¥å±‚"""
        self.lm_head = new_embeddings


class InstructionDataset(Dataset):
    """æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†"""

    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        item = self.data[idx]

        # æ„å»ºæŒ‡ä»¤æ ¼å¼çš„å®Œæ•´æ–‡æœ¬ï¼ˆè¾“å…¥+è¾“å‡ºï¼‰
        instruction = item["instruction"]
        input_text = item["input"]
        output_text = item["output"]

        # æ„å»ºå®Œæ•´çš„è®­ç»ƒæ–‡æœ¬ï¼šæŒ‡ä»¤ + è¾“å…¥ + è¾“å‡º
        full_text = f"æŒ‡ä»¤ï¼š{instruction}\nè¾“å…¥ï¼š{input_text}\nè¾“å‡ºï¼š{output_text}"

        # ç¼–ç æ–‡æœ¬
        encoding = self.tokenizer(
            full_text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()  # å¯¹äºè¯­è¨€å»ºæ¨¡ï¼Œæ ‡ç­¾å°±æ˜¯input_ids
        }


def prepare_all_data():
    """å‡†å¤‡æ‰€æœ‰æ•°æ®é›†å¹¶æ‰“ä¹±"""
    # åˆå¹¶æ‰€æœ‰æ•°æ®é›†
    all_data = question_classifier + question_type_classifier + question_answer + file_name_pick

    # æ‰“ä¹±æ•°æ®ä»¥è·å¾—æ›´å¥½çš„è®­ç»ƒæ•ˆæœ
    random.shuffle(all_data)

    logger.info(f"æ€»å…±å‡†å¤‡äº† {len(all_data)} ä¸ªè®­ç»ƒæ ·æœ¬")
    logger.info(f"é—®é¢˜åœºæ™¯åˆ†ç±»: {len(question_classifier)} ä¸ª")
    logger.info(f"é—®é¢˜ç±»å‹åˆ†ç±»: {len(question_type_classifier)} ä¸ª")
    logger.info(f"é—®é¢˜å›ç­”: {len(question_answer)} ä¸ª")
    logger.info(f"æ–‡ä»¶åæå–: {len(file_name_pick)} ä¸ª")
    logger.info("æ•°æ®å·²æ‰“ä¹±")

    return all_data


def train_model():
    """è®­ç»ƒæ¨¡å‹"""
    # æ¨¡å‹è·¯å¾„é…ç½®
    bert_model_path = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea"
    lora_path = r"E:\project\llm\lora\peft_lora_data_platform"

    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(lora_path, exist_ok=True)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åŠ è½½tokenizerå’Œæ¨¡å‹
    tokenizer = BertTokenizer.from_pretrained(bert_model_path)
    # æ·»åŠ ç‰¹æ®Štokenä»¥æ”¯æŒæŒ‡ä»¤å¾®è°ƒ
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.unk_token

    # åŠ è½½åŸºç¡€BERTæ¨¡å‹
    bert_model = BertModel.from_pretrained(bert_model_path)

    # ä½¿ç”¨è‡ªå®šä¹‰åŒ…è£…å™¨
    model = BertForInstructionTuning(bert_model)

    # é…ç½®LoRA - ä½¿ç”¨ç®€åŒ–çš„ç›®æ ‡æ¨¡å—åç§°
    lora_config = LoraConfig(
        r=16,  # LoRAç§©
        lora_alpha=32,  # LoRAç¼©æ”¾å› å­
        target_modules=["query", "key", "value", "dense"],  # ç›®æ ‡æ¨¡å—
        lora_dropout=0.1,  # LoRA dropout
        bias="none",  # åç½®è®¾ç½®
        task_type=TaskType.CAUSAL_LM,  # å› æœè¯­è¨€å»ºæ¨¡ä»»åŠ¡
    )

    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    model.to(device)

    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    model.print_trainable_parameters()

    # å‡†å¤‡æ•°æ®
    logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
    train_data = prepare_all_data()

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = InstructionDataset(train_data, tokenizer, max_length=512)

    # æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=lora_path,
        num_train_epochs=3,  # è®­ç»ƒè½®æ•°
        per_device_train_batch_size=8,  # æ‰¹æ¬¡å¤§å°
        per_device_eval_batch_size=8,
        warmup_steps=100,  # é¢„çƒ­æ­¥æ•°
        weight_decay=0.01,  # æƒé‡è¡°å‡
        logging_dir=os.path.join(lora_path, 'logs'),
        logging_steps=50,  # æ—¥å¿—æ­¥æ•°
        save_steps=500,  # ä¿å­˜æ­¥æ•°
        save_strategy="steps",
        learning_rate=2e-4,  # å­¦ä¹ ç‡
        fp16=torch.cuda.is_available(),  # æ··åˆç²¾åº¦
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        report_to=None,  # ç¦ç”¨wandbç­‰æŠ¥å‘Š
        load_best_model_at_end=False,  # ä¸éœ€è¦åŠ è½½æœ€ä½³æ¨¡å‹
    )

    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )

    logger.info("å¼€å§‹è®­ç»ƒ...")

    # å¼€å§‹è®­ç»ƒ
    trainer.train()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(lora_path)

    # ä¿å­˜é…ç½®ä¿¡æ¯
    config = {
        "bert_model_path": bert_model_path,
        "lora_config": {
            "r": 16,
            "lora_alpha": 32,
            "lora_dropout": 0.1,
            "target_modules": ["query", "key", "value", "dense"]
        },
        "training_info": {
            "total_samples": len(train_data),
            "epochs": 3,
            "batch_size": 8,
            "learning_rate": 2e-4
        }
    }

    config_path = os.path.join(lora_path, "training_config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)

    logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {lora_path}")
    logger.info(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ€»æ•°: {len(train_data)}")

    return lora_path


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ PEFT LoRA æ•°æ®å¹³å°è®­ç»ƒç³»ç»Ÿ")
    print("åŸºäºBERT-base-chineseçš„æŒ‡ä»¤å¾®è°ƒ")
    print("=" * 60)

    try:
        model_path = train_model()
        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
        print("ğŸ‰ å¯ä»¥å¼€å§‹ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹äº†ï¼")
        print("=" * 60)

    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback

        traceback.print_exc()
