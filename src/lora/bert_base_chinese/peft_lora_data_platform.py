"""
åŸºäºPEFTåº“çš„BERT-base-chineseæŒ‡ä»¤å¾®è°ƒè®­ç»ƒå™¨
ä¸€æ¬¡æ€§è®­ç»ƒï¼Œä½¿ç”¨æ‰€æœ‰æ•°æ®é›†ï¼Œç”Ÿæˆå•ä¸ªLoRAæ¨¡å‹
"""

import os
import json
import torch
import random
import numpy as np
from torch.utils.data import Dataset, DataLoader
from transformers import (
    BertTokenizer, 
    BertModel,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from peft import (
    LoraConfig,
    get_peft_model,
    TaskType
)
import logging
from tqdm import tqdm
from dataset import question_classifier, question_type_classifier, question_answer, file_name_pick

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)

set_seed(42)


class InstructionDataset(Dataset):
    """æŒ‡ä»¤å¾®è°ƒæ•°æ®é›†"""
    
    def __init__(self, data, tokenizer, max_length=512):
        self.data = data
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        item = self.data[idx]
        
        # æ„å»ºæŒ‡ä»¤æ ¼å¼çš„å®Œæ•´æ–‡æœ¬ï¼ˆè¾“å…¥+è¾“å‡ºï¼‰
        instruction = item["instruction"]
        input_text = item["input"]
        output_text = item["output"]
        
        # æ„å»ºå®Œæ•´çš„è®­ç»ƒæ–‡æœ¬ï¼šæŒ‡ä»¤ + è¾“å…¥ + è¾“å‡º
        full_text = f"æŒ‡ä»¤ï¼š{instruction}\nè¾“å…¥ï¼š{input_text}\nè¾“å‡ºï¼š{output_text}"
        
        # ç¼–ç æ–‡æœ¬
        encoding = self.tokenizer(
            full_text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': encoding['input_ids'].flatten()  # å¯¹äºè¯­è¨€å»ºæ¨¡ï¼Œæ ‡ç­¾å°±æ˜¯input_ids
        }


def prepare_all_data():
    """å‡†å¤‡æ‰€æœ‰æ•°æ®é›†å¹¶æ‰“ä¹±"""
    # åˆå¹¶æ‰€æœ‰æ•°æ®é›†
    all_data = question_classifier + question_type_classifier + question_answer + file_name_pick
    
    # æ‰“ä¹±æ•°æ®ä»¥è·å¾—æ›´å¥½çš„è®­ç»ƒæ•ˆæœ
    random.shuffle(all_data)
    
    logger.info(f"æ€»å…±å‡†å¤‡äº† {len(all_data)} ä¸ªè®­ç»ƒæ ·æœ¬")
    logger.info(f"é—®é¢˜åœºæ™¯åˆ†ç±»: {len(question_classifier)} ä¸ª")
    logger.info(f"é—®é¢˜ç±»å‹åˆ†ç±»: {len(question_type_classifier)} ä¸ª") 
    logger.info(f"é—®é¢˜å›ç­”: {len(question_answer)} ä¸ª")
    logger.info(f"æ–‡ä»¶åæå–: {len(file_name_pick)} ä¸ª")
    logger.info("æ•°æ®å·²æ‰“ä¹±")
    
    return all_data


def train_model():
    """è®­ç»ƒæ¨¡å‹"""
    # æ¨¡å‹è·¯å¾„é…ç½®
    bert_model_path = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea"
    lora_path = r"E:\project\llm\lora\peft_lora_data_platform"
    
    # åˆ›å»ºä¿å­˜ç›®å½•
    os.makedirs(lora_path, exist_ok=True)
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    logger.info(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # åŠ è½½tokenizerå’Œæ¨¡å‹
    tokenizer = BertTokenizer.from_pretrained(bert_model_path)
    # æ·»åŠ ç‰¹æ®Štokenä»¥æ”¯æŒæŒ‡ä»¤å¾®è°ƒ
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    model = BertModel.from_pretrained(bert_model_path)
    
    # é…ç½®LoRA
    lora_config = LoraConfig(
        r=16,                    # LoRAç§©
        lora_alpha=32,           # LoRAç¼©æ”¾å› å­
        target_modules=["query", "key", "value", "dense"],  # ç›®æ ‡æ¨¡å—
        lora_dropout=0.1,        # LoRA dropout
        bias="none",             # åç½®è®¾ç½®
        task_type=TaskType.FEATURE_EXTRACTION,  # ç‰¹å¾æå–ä»»åŠ¡
    )
    
    # åº”ç”¨LoRA
    model = get_peft_model(model, lora_config)
    model.to(device)
    
    # æ‰“å°å¯è®­ç»ƒå‚æ•°ä¿¡æ¯
    model.print_trainable_parameters()
    
    # å‡†å¤‡æ•°æ®
    logger.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
    train_data = prepare_all_data()
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = InstructionDataset(train_data, tokenizer, max_length=512)
    
    # æ•°æ®æ”¶é›†å™¨
    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)
    
    # è®­ç»ƒå‚æ•°
    training_args = TrainingArguments(
        output_dir=lora_path,
        num_train_epochs=3,                    # è®­ç»ƒè½®æ•°
        per_device_train_batch_size=8,         # æ‰¹æ¬¡å¤§å°
        per_device_eval_batch_size=8,
        warmup_steps=100,                      # é¢„çƒ­æ­¥æ•°
        weight_decay=0.01,                     # æƒé‡è¡°å‡
        logging_dir=os.path.join(lora_path, 'logs'),
        logging_steps=50,                      # æ—¥å¿—æ­¥æ•°
        save_steps=500,                        # ä¿å­˜æ­¥æ•°
        save_strategy="steps",
        learning_rate=2e-4,                    # å­¦ä¹ ç‡
        fp16=torch.cuda.is_available(),        # æ··åˆç²¾åº¦
        dataloader_pin_memory=False,
        remove_unused_columns=False,
        report_to=None,                        # ç¦ç”¨wandbç­‰æŠ¥å‘Š
        load_best_model_at_end=False,          # ä¸éœ€è¦åŠ è½½æœ€ä½³æ¨¡å‹
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        tokenizer=tokenizer,
        data_collator=data_collator,
    )
    
    logger.info("å¼€å§‹è®­ç»ƒ...")
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train()
    
    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    logger.info("ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹...")
    trainer.save_model()
    tokenizer.save_pretrained(lora_path)
    
    # ä¿å­˜é…ç½®ä¿¡æ¯
    config = {
        "bert_model_path": bert_model_path,
        "lora_config": {
            "r": 16,
            "lora_alpha": 32,
            "lora_dropout": 0.1,
            "target_modules": ["query", "key", "value", "dense"]
        },
        "training_info": {
            "total_samples": len(train_data),
            "epochs": 3,
            "batch_size": 8,
            "learning_rate": 2e-4
        }
    }
    
    config_path = os.path.join(lora_path, "training_config.json")
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    logger.info(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {lora_path}")
    logger.info(f"ğŸ“Š è®­ç»ƒæ ·æœ¬æ€»æ•°: {len(train_data)}")
    
    return lora_path


if __name__ == "__main__":
    print("=" * 60)
    print("ğŸš€ PEFT LoRA æ•°æ®å¹³å°è®­ç»ƒç³»ç»Ÿ")
    print("åŸºäºBERT-base-chineseçš„æŒ‡ä»¤å¾®è°ƒ")
    print("=" * 60)
    
    try:
        model_path = train_model()
        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒæˆåŠŸå®Œæˆï¼")
        print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {model_path}")
        print("ğŸ‰ å¯ä»¥å¼€å§‹ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹äº†ï¼")
        print("=" * 60)
        
    except Exception as e:
        print(f"âŒ è®­ç»ƒå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
