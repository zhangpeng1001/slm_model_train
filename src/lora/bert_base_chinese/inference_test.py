"""
PEFT LoRA æ•°æ®å¹³å°æ¨ç†æµ‹è¯•ç³»ç»Ÿ
å®ç°å®Œæ•´çš„æ¨ç†æµç¨‹ï¼šé—®é¢˜åœºæ™¯åˆ†ç±» -> æ•°æ®å¹³å°ç›¸å…³é—®é¢˜å¤„ç† -> é—®é¢˜ç±»å‹åˆ†ç±» -> å…·ä½“å¤„ç†
"""

import os
import json
import torch
import re
from transformers import BertTokenizer, BertModel
from peft import PeftModel
import logging
from typing import Dict, Any, Tuple
from peft_lora_data_platform import BertForInstructionTuning

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class DataPlatformInferenceSystem:
    """æ•°æ®å¹³å°æ¨ç†ç³»ç»Ÿ"""

    def __init__(self, model_path: str, bert_model_path: str):
        """
        åˆå§‹åŒ–æ¨ç†ç³»ç»Ÿ
        
        Args:
            model_path: LoRAæ¨¡å‹è·¯å¾„
            bert_model_path: BERTåŸºç¡€æ¨¡å‹è·¯å¾„
        """
        self.model_path = model_path
        self.bert_model_path = bert_model_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åŠ è½½æ¨¡å‹å’Œtokenizer
        self._load_model()

        logger.info(f"æ¨ç†ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨è®¾å¤‡: {self.device}")

    def _load_model(self):
        """åŠ è½½æ¨¡å‹å’Œtokenizer"""
        try:
            # åŠ è½½tokenizer
            self.tokenizer = BertTokenizer.from_pretrained(self.model_path)
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.unk_token

            # åŠ è½½åŸºç¡€BERTæ¨¡å‹
            bert_model = BertModel.from_pretrained(self.bert_model_path)

            # ä½¿ç”¨è‡ªå®šä¹‰åŒ…è£…å™¨
            base_model = BertForInstructionTuning(bert_model)

            # åŠ è½½LoRAæ¨¡å‹
            self.model = PeftModel.from_pretrained(base_model, self.model_path)
            self.model.to(self.device)
            self.model.eval()

            logger.info("æ¨¡å‹åŠ è½½æˆåŠŸ")

        except Exception as e:
            logger.error(f"æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            raise

    def _generate_response(self, instruction: str, input_text: str, max_length: int = 512) -> str:
        """
        ç”Ÿæˆæ¨¡å‹å“åº”
        
        Args:
            instruction: æŒ‡ä»¤
            input_text: è¾“å…¥æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦
            
        Returns:
            ç”Ÿæˆçš„å“åº”æ–‡æœ¬
        """
        try:
            # æ„å»ºè¾“å…¥æ–‡æœ¬
            prompt = f"æŒ‡ä»¤ï¼š{instruction}\nè¾“å…¥ï¼š{input_text}\nè¾“å‡ºï¼š"

            # ç¼–ç è¾“å…¥
            inputs = self.tokenizer(
                prompt,
                return_tensors="pt",
                max_length=max_length,
                truncation=True,
                padding=True
            )

            # ç§»åŠ¨åˆ°è®¾å¤‡
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            with torch.no_grad():
                # è·å–æ¨¡å‹è¾“å‡º
                outputs = self.model(**inputs)
                logits = outputs.logits

                # è·å–è¾“å‡ºéƒ¨åˆ†çš„logitsï¼ˆä»"è¾“å‡ºï¼š"ä¹‹åå¼€å§‹ï¼‰
                output_start_pos = len(self.tokenizer.encode(prompt, add_special_tokens=False))

                # ç®€å•çš„è´ªå¿ƒè§£ç 
                predicted_ids = torch.argmax(logits[0], dim=-1)

                # è§£ç é¢„æµ‹çš„token
                predicted_text = self.tokenizer.decode(predicted_ids, skip_special_tokens=True)

                # æå–è¾“å‡ºéƒ¨åˆ†
                if "è¾“å‡ºï¼š" in predicted_text:
                    response = predicted_text.split("è¾“å‡ºï¼š")[-1].strip()
                else:
                    response = predicted_text.strip()

                # æ¸…ç†å“åº”æ–‡æœ¬
                response = self._clean_response(response)

                return response

        except Exception as e:
            logger.error(f"ç”Ÿæˆå“åº”å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼Œç”Ÿæˆå“åº”æ—¶å‡ºç°é”™è¯¯ã€‚"

    def _clean_response(self, response: str) -> str:
        """æ¸…ç†å“åº”æ–‡æœ¬"""
        # ç§»é™¤ç‰¹æ®Štoken
        response = response.replace("[CLS]", "").replace("[SEP]", "").replace("[PAD]", "")

        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        response = re.sub(r'\s+', ' ', response).strip()

        # å¦‚æœå“åº”ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè¿”å›é»˜è®¤å“åº”
        if len(response) < 2:
            return "æ— æ³•ç”Ÿæˆæœ‰æ•ˆå“åº”"

        return response

    def classify_question_scenario(self, question: str) -> str:
        """
        é—®é¢˜åœºæ™¯åˆ†ç±»
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            åˆ†ç±»ç»“æœï¼š'æ•°æ®å¹³å°ç›¸å…³é—®é¢˜'ã€'é€šç”¨é—®é¢˜'ã€'æ— å…³é—®é¢˜'
        """
        logger.info(f"è¿›è¡Œé—®é¢˜åœºæ™¯åˆ†ç±»: {question}")

        response = self._generate_response("é—®é¢˜åœºæ™¯åˆ†ç±»", question)

        logger.info(f"åœºæ™¯åˆ†ç±»ç»“æœ: {response}")
        return response

    def classify_question_type(self, question: str) -> str:
        """
        é—®é¢˜ç±»å‹åˆ†ç±»ï¼ˆä»…å¯¹æ•°æ®å¹³å°ç›¸å…³é—®é¢˜ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            åˆ†ç±»ç»“æœï¼š'é—®é¢˜å›ç­”'ã€'ä»»åŠ¡å¤„ç†'
        """
        logger.info(f"è¿›è¡Œé—®é¢˜ç±»å‹åˆ†ç±»: {question}")

        response = self._generate_response("é—®é¢˜ç±»å‹åˆ†ç±»", question)

        logger.info(f"ç±»å‹åˆ†ç±»ç»“æœ: {response}")
        return response

    def answer_question(self, question: str) -> str:
        """
        å›ç­”é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            é—®é¢˜ç­”æ¡ˆ
        """
        logger.info(f"å›ç­”é—®é¢˜: {question}")

        response = self._generate_response("é—®é¢˜å›ç­”", question)

        logger.info(f"é—®é¢˜ç­”æ¡ˆ: {response}")
        return response

    def extract_filename(self, question: str) -> str:
        """
        æå–æ–‡ä»¶åç§°
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            æå–çš„æ–‡ä»¶åç§°
        """
        logger.info(f"æå–æ–‡ä»¶åç§°: {question}")

        response = self._generate_response("æ–‡ä»¶åç§°æå–", question)

        logger.info(f"æå–çš„æ–‡ä»¶åç§°: {response}")
        return response

    def process_question(self, question: str) -> Dict[str, Any]:
        """
        å®Œæ•´çš„é—®é¢˜å¤„ç†æµç¨‹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        result = {
            "question": question,
            "scenario": None,
            "question_type": None,
            "response": None,
            "filename": None,
            "processing_steps": []
        }

        try:
            # æ­¥éª¤1: é—®é¢˜åœºæ™¯åˆ†ç±»
            result["processing_steps"].append("1. é—®é¢˜åœºæ™¯åˆ†ç±»")
            scenario = self.classify_question_scenario(question)
            result["scenario"] = scenario

            # æ­¥éª¤2: æ ¹æ®åœºæ™¯è¿›è¡Œä¸åŒå¤„ç†
            if "æ•°æ®å¹³å°ç›¸å…³é—®é¢˜" in scenario:
                result["processing_steps"].append("2. è¯†åˆ«ä¸ºæ•°æ®å¹³å°ç›¸å…³é—®é¢˜ï¼Œç»§ç»­åˆ†ç±»")

                # æ­¥éª¤3: é—®é¢˜ç±»å‹åˆ†ç±»
                result["processing_steps"].append("3. é—®é¢˜ç±»å‹åˆ†ç±»")
                question_type = self.classify_question_type(question)
                result["question_type"] = question_type

                # æ­¥éª¤4: æ ¹æ®é—®é¢˜ç±»å‹è¿›è¡Œå¤„ç†
                if "é—®é¢˜å›ç­”" in question_type:
                    result["processing_steps"].append("4. é—®é¢˜å›ç­”ç±»å‹ï¼Œè°ƒç”¨é—®é¢˜å›ç­”")
                    answer = self.answer_question(question)
                    result["response"] = answer

                elif "ä»»åŠ¡å¤„ç†" in question_type:
                    result["processing_steps"].append("4. ä»»åŠ¡å¤„ç†ç±»å‹ï¼Œæå–æ–‡ä»¶åç§°")
                    filename = self.extract_filename(question)
                    result["filename"] = filename
                    result["response"] = f"å·²è¯†åˆ«ä»»åŠ¡å¤„ç†è¯·æ±‚ï¼Œç›¸å…³æ–‡ä»¶ï¼š{filename}ã€‚è¯·ç¡®è®¤æ˜¯å¦éœ€è¦å¤„ç†æ­¤æ–‡ä»¶ã€‚"

                else:
                    result["processing_steps"].append("4. æœªè¯†åˆ«çš„é—®é¢˜ç±»å‹ï¼Œä½¿ç”¨é»˜è®¤å›ç­”")
                    result["response"] = "æŠ±æ­‰ï¼Œæ— æ³•è¯†åˆ«å…·ä½“çš„é—®é¢˜ç±»å‹ï¼Œè¯·æä¾›æ›´è¯¦ç»†çš„ä¿¡æ¯ã€‚"

            elif "é€šç”¨é—®é¢˜" in scenario:
                result["processing_steps"].append("2. è¯†åˆ«ä¸ºé€šç”¨é—®é¢˜ï¼Œä½¿ç”¨é€šç”¨å›å¤")
                result["response"] = self.generate_general_response(question)

            elif "æ— å…³é—®é¢˜" in scenario:
                result["processing_steps"].append("2. è¯†åˆ«ä¸ºæ— å…³é—®é¢˜ï¼Œæç¤ºç”¨æˆ·")
                result["response"] = "æŠ±æ­‰ï¼Œæˆ‘æ˜¯æ•°æ®å¹³å°ä¸“ç”¨åŠ©æ‰‹ï¼Œåªèƒ½å¤„ç†æ•°æ®å¹³å°ç›¸å…³çš„é—®é¢˜ã€‚è¯·å’¨è¯¢æ•°æ®æ¸…æ´—ã€æ•°æ®å…¥åº“ã€æ•°æ®è´¨é‡æ£€æŸ¥ç­‰ç›¸å…³é—®é¢˜ã€‚"

            else:
                result["processing_steps"].append("2. åœºæ™¯åˆ†ç±»ä¸æ˜ç¡®ï¼Œä½¿ç”¨é»˜è®¤å›å¤")
                result["response"] = "æŠ±æ­‰ï¼Œæ— æ³•å‡†ç¡®ç†è§£æ‚¨çš„é—®é¢˜ï¼Œè¯·é‡æ–°æè¿°æˆ–æä¾›æ›´å¤šä¿¡æ¯ã€‚"

        except Exception as e:
            logger.error(f"é—®é¢˜å¤„ç†å¤±è´¥: {e}")
            result["response"] = "å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åå†è¯•ã€‚"
            result["processing_steps"].append(f"é”™è¯¯: {str(e)}")

        return result

    def generate_general_response(self, question: str) -> str:
        """
        ç”Ÿæˆé€šç”¨å›ç­”ï¼ˆå¯¹äºéæ•°æ®å¹³å°ç›¸å…³é—®é¢˜ï¼‰
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            é€šç”¨å›ç­”
        """
        # ç®€å•çš„é€šç”¨å›ç­”é€»è¾‘
        if any(greeting in question.lower() for greeting in ["ä½ å¥½", "æ‚¨å¥½", "æ—©ä¸Šå¥½", "ä¸‹åˆå¥½"]):
            return "æ‚¨å¥½ï¼æˆ‘æ˜¯æ•°æ®å¹³å°æ™ºèƒ½åŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚"
        elif any(thanks in question.lower() for thanks in ["è°¢è°¢", "æ„Ÿè°¢"]):
            return "ä¸å®¢æ°”ï¼å¦‚æœè¿˜æœ‰å…¶ä»–æ•°æ®å¹³å°ç›¸å…³é—®é¢˜ï¼Œéšæ—¶å¯ä»¥å’¨è¯¢æˆ‘ã€‚"
        elif any(bye in question.lower() for bye in ["å†è§", "æ‹œæ‹œ"]):
            return "å†è§ï¼ç¥æ‚¨å·¥ä½œé¡ºåˆ©ï¼"
        else:
            return "æˆ‘æ˜¯æ•°æ®å¹³å°ä¸“ç”¨åŠ©æ‰‹ï¼Œä¸»è¦å¤„ç†æ•°æ®æ¸…æ´—ã€æ•°æ®å…¥åº“ã€æ•°æ®è´¨é‡æ£€æŸ¥ç­‰ç›¸å…³é—®é¢˜ã€‚è¯·é—®æœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"


class InferenceTestRunner:
    """æ¨ç†æµ‹è¯•è¿è¡Œå™¨"""

    def __init__(self, inference_system: DataPlatformInferenceSystem):
        """
        åˆå§‹åŒ–æµ‹è¯•è¿è¡Œå™¨
        
        Args:
            inference_system: æ¨ç†ç³»ç»Ÿå®ä¾‹
        """
        self.inference_system = inference_system

    def run_scenario_tests(self):
        """è¿è¡Œ4ç§åœºæ™¯çš„æµ‹è¯•"""
        print("\n" + "=" * 80)
        print("ğŸ§ª å¼€å§‹è¿è¡Œ4ç§åœºæ™¯çš„æ¨ç†æµ‹è¯•")
        print("=" * 80)

        # æµ‹è¯•ç”¨ä¾‹
        test_cases = [
            # åœºæ™¯1: æ•°æ®å¹³å°ç›¸å…³é—®é¢˜ - é—®é¢˜å›ç­”ç±»å‹
            {
                "category": "æ•°æ®å¹³å°ç›¸å…³é—®é¢˜ - é—®é¢˜å›ç­”",
                "questions": [
                    "æ•°æ®æ¸…æ´—æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
                    "å¦‚ä½•è¿›è¡Œæ•°æ®å…¥åº“ï¼Ÿ",
                    "æ•°æ®è´¨é‡æ£€æŸ¥åŒ…æ‹¬å“ªäº›å†…å®¹ï¼Ÿ",
                    "æ•°æ®ç›‘æ§æ€ä¹ˆåšï¼Ÿ",
                    "å¦‚ä½•ä¿è¯æ•°æ®å®‰å…¨ï¼Ÿ"
                ]
            },

            # åœºæ™¯2: æ•°æ®å¹³å°ç›¸å…³é—®é¢˜ - ä»»åŠ¡å¤„ç†ç±»å‹
            {
                "category": "æ•°æ®å¹³å°ç›¸å…³é—®é¢˜ - ä»»åŠ¡å¤„ç†",
                "questions": [
                    "è¯·å¸®æˆ‘æŠŠå®æ™¯ä¸‰ç»´æ¨¡å‹æˆæœæ•°æ®è¿›è¡Œæ²»ç†",
                    "æˆ‘æœ‰ä¸€æ‰¹è¥¿å®‰å¸‚åœ°ç±»å›¾æ–‘çš„æ•°æ®ï¼Œæ€ä¹ˆè¿›è¡Œå‘æœåŠ¡",
                    "æˆ‘å·²ç»ä¸Šä¼ äº†å•æ³¢æ®µæµ®ç‚¹æŠ•å½±çš„æ•°æ®ï¼Œç°åœ¨æƒ³è¿›è¡Œå…¥åº“",
                    "æœ‰ä¸€äº›é¥æ„Ÿå½±åƒæ•°æ®éœ€è¦å¤„ç†",
                    "DEMé«˜ç¨‹æ•°æ®éœ€è¦è¿›è¡Œè´¨é‡æ£€æŸ¥"
                ]
            },

            # åœºæ™¯3: é€šç”¨é—®é¢˜
            {
                "category": "é€šç”¨é—®é¢˜",
                "questions": [
                    "ä½ å¥½",
                    "æ‚¨å¥½ï¼Œè¯·é—®",
                    "è°¢è°¢ä½ çš„å¸®åŠ©",
                    "å†è§",
                    "æ—©ä¸Šå¥½"
                ]
            },

            # åœºæ™¯4: æ— å…³é—®é¢˜
            {
                "category": "æ— å…³é—®é¢˜",
                "questions": [
                    "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
                    "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½åƒçš„ï¼Ÿ",
                    "å¦‚ä½•å­¦ä¹ è‹±è¯­ï¼Ÿ",
                    "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
                    "æ¨èä¸€éƒ¨ç”µå½±"
                ]
            }
        ]

        # è¿è¡Œæµ‹è¯•
        total_tests = 0
        successful_tests = 0

        for test_case in test_cases:
            print(f"\nğŸ“‹ æµ‹è¯•åœºæ™¯: {test_case['category']}")
            print("-" * 60)

            for i, question in enumerate(test_case['questions'], 1):
                total_tests += 1
                print(f"\nğŸ” æµ‹è¯• {i}: {question}")

                try:
                    # æ‰§è¡Œå®Œæ•´çš„é—®é¢˜å¤„ç†æµç¨‹
                    result = self.inference_system.process_question(question)

                    # æ˜¾ç¤ºå¤„ç†ç»“æœ
                    print(f"ğŸ“Š å¤„ç†ç»“æœ:")
                    print(f"   é—®é¢˜åœºæ™¯: {result['scenario']}")
                    if result['question_type']:
                        print(f"   é—®é¢˜ç±»å‹: {result['question_type']}")
                    if result['filename']:
                        print(f"   æå–æ–‡ä»¶å: {result['filename']}")
                    print(f"   æœ€ç»ˆå›å¤: {result['response']}")

                    # æ˜¾ç¤ºå¤„ç†æ­¥éª¤
                    print(f"ğŸ”„ å¤„ç†æ­¥éª¤:")
                    for step in result['processing_steps']:
                        print(f"   {step}")

                    successful_tests += 1
                    print("âœ… æµ‹è¯•æˆåŠŸ")

                except Exception as e:
                    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")

                print("-" * 40)

        # æ˜¾ç¤ºæµ‹è¯•æ€»ç»“
        print(f"\nğŸ“ˆ æµ‹è¯•æ€»ç»“:")
        print(f"   æ€»æµ‹è¯•æ•°: {total_tests}")
        print(f"   æˆåŠŸæ•°: {successful_tests}")
        print(f"   å¤±è´¥æ•°: {total_tests - successful_tests}")
        print(f"   æˆåŠŸç‡: {successful_tests / total_tests * 100:.1f}%")

    def run_interactive_test(self):
        """è¿è¡Œäº¤äº’å¼æµ‹è¯•"""
        print("\n" + "=" * 80)
        print("ğŸ¯ äº¤äº’å¼æ¨ç†æµ‹è¯•")
        print("è¾“å…¥ 'quit' æˆ– 'exit' é€€å‡ºæµ‹è¯•")
        print("=" * 80)

        while True:
            try:
                question = input("\nè¯·è¾“å…¥æ‚¨çš„é—®é¢˜: ").strip()

                if question.lower() in ['quit', 'exit', 'é€€å‡º']:
                    print("ğŸ‘‹ é€€å‡ºäº¤äº’å¼æµ‹è¯•")
                    break

                if not question:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜")
                    continue

                print(f"\nğŸ” å¤„ç†é—®é¢˜: {question}")
                print("-" * 50)

                # æ‰§è¡Œå®Œæ•´çš„é—®é¢˜å¤„ç†æµç¨‹
                result = self.inference_system.process_question(question)

                # æ˜¾ç¤ºå¤„ç†ç»“æœ
                print(f"\nğŸ“Š å¤„ç†ç»“æœ:")
                print(f"   é—®é¢˜åœºæ™¯: {result['scenario']}")
                if result['question_type']:
                    print(f"   é—®é¢˜ç±»å‹: {result['question_type']}")
                if result['filename']:
                    print(f"   æå–æ–‡ä»¶å: {result['filename']}")
                print(f"   æœ€ç»ˆå›å¤: {result['response']}")

                # æ˜¾ç¤ºå¤„ç†æ­¥éª¤
                print(f"\nğŸ”„ å¤„ç†æ­¥éª¤:")
                for step in result['processing_steps']:
                    print(f"   {step}")

            except KeyboardInterrupt:
                print("\nğŸ‘‹ é€€å‡ºäº¤äº’å¼æµ‹è¯•")
                break
            except Exception as e:
                print(f"âŒ å¤„ç†å¤±è´¥: {e}")

    def run_single_step_tests(self):
        """è¿è¡Œå•æ­¥åŠŸèƒ½æµ‹è¯•"""
        print("\n" + "=" * 80)
        print("ğŸ”§ å•æ­¥åŠŸèƒ½æµ‹è¯•")
        print("=" * 80)

        test_questions = [
            "æ•°æ®æ¸…æ´—æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "è¯·å¸®æˆ‘æŠŠå®æ™¯ä¸‰ç»´æ¨¡å‹æˆæœæ•°æ®è¿›è¡Œæ²»ç†",
            "ä½ å¥½",
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ"
        ]

        for question in test_questions:
            print(f"\nğŸ” æµ‹è¯•é—®é¢˜: {question}")
            print("-" * 50)

            # æµ‹è¯•é—®é¢˜åœºæ™¯åˆ†ç±»
            print("1ï¸âƒ£ é—®é¢˜åœºæ™¯åˆ†ç±»:")
            scenario = self.inference_system.classify_question_scenario(question)
            print(f"   ç»“æœ: {scenario}")

            # å¦‚æœæ˜¯æ•°æ®å¹³å°ç›¸å…³é—®é¢˜ï¼Œç»§ç»­æµ‹è¯•
            if "æ•°æ®å¹³å°ç›¸å…³é—®é¢˜" in scenario:
                print("2ï¸âƒ£ é—®é¢˜ç±»å‹åˆ†ç±»:")
                question_type = self.inference_system.classify_question_type(question)
                print(f"   ç»“æœ: {question_type}")

                if "é—®é¢˜å›ç­”" in question_type:
                    print("3ï¸âƒ£ é—®é¢˜å›ç­”:")
                    answer = self.inference_system.answer_question(question)
                    print(f"   ç»“æœ: {answer}")

                elif "ä»»åŠ¡å¤„ç†" in question_type:
                    print("3ï¸âƒ£ æ–‡ä»¶åæå–:")
                    filename = self.inference_system.extract_filename(question)
                    print(f"   ç»“æœ: {filename}")

            print("=" * 50)


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ PEFT LoRA æ•°æ®å¹³å°æ¨ç†æµ‹è¯•ç³»ç»Ÿ")
    print("=" * 60)

    # æ¨¡å‹è·¯å¾„é…ç½®
    bert_model_path = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea"
    lora_model_path = r"E:\project\llm\lora\peft_lora_mixed"

    try:
        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not os.path.exists(bert_model_path):
            print(f"âŒ BERTæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {bert_model_path}")
            return

        if not os.path.exists(lora_model_path):
            print(f"âŒ LoRAæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {lora_model_path}")
            return

        print("ğŸ“¦ åˆå§‹åŒ–æ¨ç†ç³»ç»Ÿ...")

        # åˆå§‹åŒ–æ¨ç†ç³»ç»Ÿ
        inference_system = DataPlatformInferenceSystem(
            model_path=lora_model_path,
            bert_model_path=bert_model_path
        )

        # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
        test_runner = InferenceTestRunner(inference_system)

        print("\nğŸ¯ é€‰æ‹©æµ‹è¯•æ¨¡å¼:")
        print("1. å®Œæ•´åœºæ™¯æµ‹è¯• (æµ‹è¯•4ç§åœºæ™¯çš„æ‰€æœ‰ç”¨ä¾‹)")
        print("2. å•æ­¥åŠŸèƒ½æµ‹è¯• (æµ‹è¯•å„ä¸ªåŠŸèƒ½æ¨¡å—)")
        print("3. äº¤äº’å¼æµ‹è¯• (æ‰‹åŠ¨è¾“å…¥é—®é¢˜æµ‹è¯•)")
        print("4. é€€å‡º")

        while True:
            try:
                choice = input("\nè¯·é€‰æ‹©æµ‹è¯•æ¨¡å¼ (1-4): ").strip()

                if choice == "1":
                    test_runner.run_scenario_tests()
                elif choice == "2":
                    test_runner.run_single_step_tests()
                elif choice == "3":
                    test_runner.run_interactive_test()
                elif choice == "4":
                    print("ğŸ‘‹ é€€å‡ºæµ‹è¯•ç³»ç»Ÿ")
                    break
                else:
                    print("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé€‰é¡¹ (1-4)")

            except KeyboardInterrupt:
                print("\nğŸ‘‹ é€€å‡ºæµ‹è¯•ç³»ç»Ÿ")
                break
            except Exception as e:
                print(f"âŒ è¿è¡Œæµ‹è¯•æ—¶å‡ºé”™: {e}")

    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
