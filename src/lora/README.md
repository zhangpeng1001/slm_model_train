# LoRAé—®é¢˜åˆ†ç±»å™¨

åŸºäºLoRAï¼ˆLow-Rank Adaptationï¼‰æŠ€æœ¯çš„BERTé—®é¢˜åˆ†ç±»å™¨å®ç°ï¼Œç”¨äºå¯¹é—®é¢˜è¿›è¡Œä¸‰åˆ†ç±»ï¼šæ•°æ®å¹³å°ç›¸å…³ã€é€šç”¨å¯¹è¯ã€æ— å…³é—®é¢˜ã€‚

## ğŸš€ ç‰¹æ€§

- **é«˜æ•ˆå¾®è°ƒ**: ä½¿ç”¨LoRAæŠ€æœ¯ï¼Œåªè®­ç»ƒå°‘é‡å‚æ•°ï¼ˆé€šå¸¸<5%ï¼‰ï¼Œå¤§å¹…å‡å°‘è®¡ç®—èµ„æºéœ€æ±‚
- **ä¿ç•™åŸæœ‰èƒ½åŠ›**: å†»ç»“é¢„è®­ç»ƒæ¨¡å‹å‚æ•°ï¼Œä¿æŒBERTçš„åŸæœ‰è¯­è¨€ç†è§£èƒ½åŠ›
- **çµæ´»é…ç½®**: æ”¯æŒè‡ªå®šä¹‰LoRAå‚æ•°ï¼ˆrankã€alphaã€dropoutï¼‰
- **å®Œæ•´æµç¨‹**: åŒ…å«è®­ç»ƒã€è¯„ä¼°ã€æ¨ç†å’Œäº¤äº’æµ‹è¯•åŠŸèƒ½
- **æ˜“äºä½¿ç”¨**: æä¾›å‘½ä»¤è¡Œå’Œäº¤äº’å¼ä¸¤ç§ä½¿ç”¨æ–¹å¼

## ğŸ“‹ ç¯å¢ƒè¦æ±‚

```bash
torch>=1.9.0
transformers>=4.20.0
scikit-learn>=1.0.0
tqdm>=4.64.0
numpy>=1.21.0
```

## ğŸ› ï¸ å®‰è£…ä¾èµ–

```bash
pip install torch transformers scikit-learn tqdm numpy
```

## ğŸ“ æ–‡ä»¶ç»“æ„

```
src/lora/
â”œâ”€â”€ lora_question_classifier.py    # ä¸»å®ç°æ–‡ä»¶
â”œâ”€â”€ test_lora_implementation.py    # æµ‹è¯•è„šæœ¬
â””â”€â”€ README.md                      # è¯´æ˜æ–‡æ¡£
```

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### LoRAæŠ€æœ¯åŸç†

LoRAï¼ˆLow-Rank Adaptationï¼‰é€šè¿‡åœ¨é¢„è®­ç»ƒæ¨¡å‹çš„çº¿æ€§å±‚ä¸­æ·»åŠ ä½ç§©çŸ©é˜µæ¥å®ç°é«˜æ•ˆå¾®è°ƒï¼š

```
h = Wâ‚€x + Î”Wx = Wâ‚€x + BAx
```

å…¶ä¸­ï¼š
- `Wâ‚€`: å†»ç»“çš„é¢„è®­ç»ƒæƒé‡
- `B`, `A`: å¯è®­ç»ƒçš„ä½ç§©çŸ©é˜µ
- `rank`: ä½ç§©çŸ©é˜µçš„ç§©ï¼Œæ§åˆ¶å‚æ•°é‡
- `alpha`: ç¼©æ”¾å› å­ï¼Œæ§åˆ¶LoRAçš„å½±å“å¼ºåº¦

### å‚æ•°é…ç½®

- **lora_rank**: LoRAçŸ©é˜µçš„ç§©ï¼ˆé»˜è®¤8ï¼‰
  - è¾ƒå°å€¼ï¼šå‚æ•°æ›´å°‘ï¼Œè®­ç»ƒæ›´å¿«ï¼Œä½†è¡¨è¾¾èƒ½åŠ›æœ‰é™
  - è¾ƒå¤§å€¼ï¼šè¡¨è¾¾èƒ½åŠ›æ›´å¼ºï¼Œä½†å‚æ•°å¢åŠ 

- **lora_alpha**: ç¼©æ”¾å› å­ï¼ˆé»˜è®¤16ï¼‰
  - æ§åˆ¶LoRAé€‚åº”çš„å¼ºåº¦
  - é€šå¸¸è®¾ä¸ºrankçš„2å€

- **lora_dropout**: Dropoutç‡ï¼ˆé»˜è®¤0.1ï¼‰
  - é˜²æ­¢è¿‡æ‹Ÿåˆ
  - æé«˜æ³›åŒ–èƒ½åŠ›

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. è®­ç»ƒæ¨¡å‹

```bash
# ä½¿ç”¨é»˜è®¤å‚æ•°è®­ç»ƒ
python lora_question_classifier.py train

# æˆ–è€…ç›´æ¥è¿è¡Œè¿›å…¥èœå•
python lora_question_classifier.py
```

### 2. æµ‹è¯•æ¨¡å‹

```bash
# æµ‹è¯•å·²è®­ç»ƒçš„æ¨¡å‹
python lora_question_classifier.py test
```

### 3. äº¤äº’å¼æµ‹è¯•

```bash
# äº¤äº’å¼é—®é¢˜åˆ†ç±»æµ‹è¯•
python lora_question_classifier.py interactive
```

### 4. è¿è¡Œæµ‹è¯•è„šæœ¬

```bash
# éªŒè¯å®ç°æ­£ç¡®æ€§
python test_lora_implementation.py
```

## ğŸ“Š ä½¿ç”¨ç¤ºä¾‹

### è®­ç»ƒè‡ªå®šä¹‰LoRAæ¨¡å‹

```python
from lora_question_classifier import LoRAQuestionClassifierTrainer

# åˆ›å»ºè®­ç»ƒå™¨
trainer = LoRAQuestionClassifierTrainer(
    model_path="path/to/bert-base-chinese",
    save_dir="./my_lora_model",
    lora_rank=16,      # æ›´å¤§çš„rank
    lora_alpha=32,     # å¯¹åº”çš„alpha
    lora_dropout=0.1
)

# è®­ç»ƒæ¨¡å‹
trainer.train(
    epochs=5,
    batch_size=8,
    learning_rate=1e-4
)

# ä¿å­˜æ¨¡å‹
trainer.save_lora_weights('final_model')
```

### ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†

```python
from lora_question_classifier import LoRAQuestionClassifier

# åŠ è½½æ¨¡å‹
classifier = LoRAQuestionClassifier(
    base_model_path="path/to/bert-base-chinese",
    lora_path="./my_lora_model/final_model"
)

# åˆ†ç±»é—®é¢˜
result = classifier.classify_question("æ•°æ®æ¸…æ´—æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ")
print(f"åˆ†ç±»: {result['category']}")
print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
```

## ğŸ›ï¸ é…ç½®è¯´æ˜

### æ¨¡å‹è·¯å¾„é…ç½®

åœ¨ä»£ç ä¸­ä¿®æ”¹BERTæ¨¡å‹è·¯å¾„ï¼š

```python
bert_model_path = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea"
```

### LoRAä¿å­˜è·¯å¾„

é»˜è®¤ä¿å­˜åˆ°ï¼š`E:/project/llm/lora/lora_question_classifier`

å¯ä»¥é€šè¿‡`save_dir`å‚æ•°è‡ªå®šä¹‰ï¼š

```python
trainer = LoRAQuestionClassifierTrainer(
    model_path=bert_model_path,
    save_dir="./custom_save_path"
)
```

## ğŸ“ˆ æ€§èƒ½å¯¹æ¯”

| æ–¹æ³• | å¯è®­ç»ƒå‚æ•° | è®­ç»ƒæ—¶é—´ | å†…å­˜å ç”¨ | å‡†ç¡®ç‡ |
|------|------------|----------|----------|--------|
| å…¨é‡å¾®è°ƒ | ~110M | é•¿ | é«˜ | é«˜ |
| LoRAå¾®è°ƒ | ~2M | çŸ­ | ä½ | æ¥è¿‘å…¨é‡ |

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æ•°æ®é›†

```python
def prepare_custom_data():
    texts = ["ä½ çš„é—®é¢˜1", "ä½ çš„é—®é¢˜2", ...]
    labels = [0, 1, 2, ...]  # 0:æ•°æ®å¹³å°, 1:é€šç”¨å¯¹è¯, 2:æ— å…³
    return texts, labels

# åœ¨è®­ç»ƒå™¨ä¸­ä½¿ç”¨
trainer.prepare_training_data = prepare_custom_data
```

### è°ƒæ•´LoRAåº”ç”¨å±‚

é»˜è®¤åº”ç”¨åˆ°attentionå±‚å’Œå‰é¦ˆç½‘ç»œï¼Œå¯ä»¥åœ¨`_apply_lora`æ–¹æ³•ä¸­è‡ªå®šä¹‰ã€‚

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **CUDAå†…å­˜ä¸è¶³**
   - å‡å°batch_size
   - å‡å°lora_rank
   - ä½¿ç”¨CPUè®­ç»ƒ

2. **æ¨¡å‹è·¯å¾„é”™è¯¯**
   - æ£€æŸ¥BERTæ¨¡å‹è·¯å¾„æ˜¯å¦æ­£ç¡®
   - ç¡®ä¿æ¨¡å‹æ–‡ä»¶å®Œæ•´

3. **ä¾èµ–ç‰ˆæœ¬å†²çª**
   - ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒ
   - æŒ‰ç…§è¦æ±‚å®‰è£…æŒ‡å®šç‰ˆæœ¬

### è°ƒè¯•æ¨¡å¼

```python
import logging
logging.basicConfig(level=logging.DEBUG)
```

## ğŸ“ æ‰©å±•å¼€å‘

### æ·»åŠ æ–°çš„åˆ†ç±»ç±»åˆ«

1. ä¿®æ”¹`label_map`
2. æ›´æ–°è®­ç»ƒæ•°æ®
3. è°ƒæ•´æ¨¡å‹è¾“å‡ºç»´åº¦

### æ”¯æŒå…¶ä»–é¢„è®­ç»ƒæ¨¡å‹

1. ä¿®æ”¹æ¨¡å‹åŠ è½½ä»£ç 
2. è°ƒæ•´LoRAåº”ç”¨å±‚
3. é€‚é…tokenizer
