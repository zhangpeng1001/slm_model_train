"""
åŸºäºLoRAæŠ€æœ¯çš„é—®é¢˜åˆ†ç±»å™¨è®­ç»ƒå™¨
ä½¿ç”¨BERT-base-chineseæ¨¡å‹è¿›è¡Œä½ç§©é€‚åº”å¾®è°ƒï¼Œå®ç°é—®é¢˜ä¸‰åˆ†ç±»ï¼šæ•°æ®å¹³å°ç›¸å…³ã€é€šç”¨å¯¹è¯ã€æ— å…³é—®é¢˜
ä¿ç•™æ¨¡å‹åŸæœ‰èƒ½åŠ›çš„åŒæ—¶è¿›è¡Œé«˜æ•ˆå¾®è°ƒ
"""
import os
import json
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import (
    BertTokenizer, BertForSequenceClassification,
    get_linear_schedule_with_warmup
)
import numpy as np
from tqdm import tqdm
import logging
from sklearn.metrics import accuracy_score, classification_report
from torch.optim import AdamW
import math

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LoRALayer(nn.Module):
    """LoRA (Low-Rank Adaptation) å±‚å®ç°"""

    def __init__(self, original_layer, rank=8, alpha=16, dropout=0.1):
        super().__init__()
        self.original_layer = original_layer
        self.rank = rank
        self.alpha = alpha
        self.scaling = alpha / rank

        # å†»ç»“åŸå§‹å±‚å‚æ•°
        for param in self.original_layer.parameters():
            param.requires_grad = False

        # è·å–åŸå§‹å±‚çš„è¾“å…¥è¾“å‡ºç»´åº¦
        if hasattr(original_layer, 'in_features') and hasattr(original_layer, 'out_features'):
            # Linearå±‚
            in_features = original_layer.in_features
            out_features = original_layer.out_features
        elif hasattr(original_layer, 'weight'):
            # å…¶ä»–å±‚ï¼Œä»æƒé‡å½¢çŠ¶æ¨æ–­
            weight_shape = original_layer.weight.shape
            if len(weight_shape) == 2:
                out_features, in_features = weight_shape
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å±‚ç±»å‹: {type(original_layer)}")
        else:
            raise ValueError(f"æ— æ³•ç¡®å®šå±‚çš„è¾“å…¥è¾“å‡ºç»´åº¦: {type(original_layer)}")

        # LoRAçš„Aå’ŒBçŸ©é˜µ
        self.lora_A = nn.Linear(in_features, rank, bias=False)
        self.lora_B = nn.Linear(rank, out_features, bias=False)
        self.dropout = nn.Dropout(dropout)

        # åˆå§‹åŒ–LoRAæƒé‡
        nn.init.kaiming_uniform_(self.lora_A.weight, a=math.sqrt(5))
        nn.init.zeros_(self.lora_B.weight)

    def forward(self, x):
        # åŸå§‹å±‚çš„è¾“å‡º
        original_output = self.original_layer(x)

        # LoRAçš„è¾“å‡º
        lora_output = self.lora_B(self.dropout(self.lora_A(x))) * self.scaling

        return original_output + lora_output


class QuestionDataset(Dataset):
    """é—®é¢˜åˆ†ç±»æ•°æ®é›†"""

    def __init__(self, texts, labels, tokenizer, max_length=128):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = self.labels[idx]

        # ç¼–ç æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )

        return {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'label': torch.tensor(label, dtype=torch.long)
        }


class LoRAQuestionClassifierTrainer:
    """åŸºäºLoRAçš„é—®é¢˜åˆ†ç±»å™¨è®­ç»ƒå™¨"""

    def __init__(self, model_path, save_dir='E:/project/llm/lora/lora_question_classifier',
                 lora_rank=8, lora_alpha=16, lora_dropout=0.1):
        self.model_path = model_path
        self.save_dir = save_dir
        self.lora_rank = lora_rank
        self.lora_alpha = lora_alpha
        self.lora_dropout = lora_dropout
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åˆ›å»ºä¿å­˜ç›®å½•
        os.makedirs(save_dir, exist_ok=True)

        # ç±»åˆ«å®šä¹‰
        self.label_map = {
            0: "data_platform",  # æ•°æ®å¹³å°ç›¸å…³
            1: "general_chat",  # é€šç”¨å¯¹è¯
            2: "irrelevant"  # æ— å…³é—®é¢˜
        }

        # åˆå§‹åŒ–tokenizerå’Œæ¨¡å‹
        self.tokenizer = BertTokenizer.from_pretrained(model_path)
        self.model = BertForSequenceClassification.from_pretrained(
            model_path,
            num_labels=3
        )

        # åº”ç”¨LoRAåˆ°æ¨¡å‹
        self._apply_lora()

        self.model.to(self.device)
        logger.info(f"LoRAåˆ†ç±»å™¨å·²åŠ è½½åˆ°è®¾å¤‡: {self.device}")
        logger.info(f"LoRAé…ç½®: rank={lora_rank}, alpha={lora_alpha}, dropout={lora_dropout}")

    def _apply_lora(self):
        """å°†LoRAåº”ç”¨åˆ°BERTæ¨¡å‹çš„å…³é”®å±‚"""
        # åº”ç”¨LoRAåˆ°BERTçš„attentionå±‚
        for layer in self.model.bert.encoder.layer:
            # Query, Key, ValueæŠ•å½±å±‚
            layer.attention.self.query = LoRALayer(
                layer.attention.self.query,
                rank=self.lora_rank,
                alpha=self.lora_alpha,
                dropout=self.lora_dropout
            )
            layer.attention.self.key = LoRALayer(
                layer.attention.self.key,
                rank=self.lora_rank,
                alpha=self.lora_alpha,
                dropout=self.lora_dropout
            )
            layer.attention.self.value = LoRALayer(
                layer.attention.self.value,
                rank=self.lora_rank,
                alpha=self.lora_alpha,
                dropout=self.lora_dropout
            )

            # å‰é¦ˆç½‘ç»œçš„ç¬¬ä¸€å±‚
            layer.intermediate.dense = LoRALayer(
                layer.intermediate.dense,
                rank=self.lora_rank,
                alpha=self.lora_alpha,
                dropout=self.lora_dropout
            )

        # åˆ†ç±»å¤´ä¿æŒå¯è®­ç»ƒ
        for param in self.model.classifier.parameters():
            param.requires_grad = True

        # ç»Ÿè®¡å¯è®­ç»ƒå‚æ•°
        total_params = sum(p.numel() for p in self.model.parameters())
        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

        logger.info(f"æ€»å‚æ•°æ•°é‡: {total_params:,}")
        logger.info(f"å¯è®­ç»ƒå‚æ•°æ•°é‡: {trainable_params:,}")
        logger.info(f"å¯è®­ç»ƒå‚æ•°æ¯”ä¾‹: {100 * trainable_params / total_params:.2f}%")

    def prepare_training_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""

        # æ•°æ®å¹³å°ç›¸å…³é—®é¢˜ï¼ˆç±»åˆ«0ï¼‰
        data_platform_questions = [
            "æ•°æ®æ¸…æ´—æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
            "å¦‚ä½•è¿›è¡Œæ•°æ®æ¸…æ´—ï¼Ÿ",
            "æ•°æ®æ¸…æ´—æ­¥éª¤æœ‰å“ªäº›ï¼Ÿ",
            "æ•°æ®æ¸…æ´—çš„æµç¨‹",
            "æ€ä¹ˆæ¸…æ´—æ•°æ®ï¼Ÿ",
            "æ•°æ®å…¥åº“æµç¨‹",
            "å¦‚ä½•è¿›è¡Œæ•°æ®å…¥åº“ï¼Ÿ",
            "æ•°æ®å…¥åº“çš„æ­¥éª¤",
            "æ€æ ·å…¥åº“æ•°æ®ï¼Ÿ",
            "æ•°æ®å…¥åº“æ“ä½œ",
            "æ•°æ®è´¨é‡æ£€æŸ¥",
            "å¦‚ä½•æ£€æŸ¥æ•°æ®è´¨é‡ï¼Ÿ",
            "æ•°æ®è´¨é‡æ£€æŸ¥æ–¹æ³•",
            "æ•°æ®è´¨é‡å¦‚ä½•ä¿è¯ï¼Ÿ",
            "è´¨é‡æ£€æŸ¥æ­¥éª¤",
            "æ•°æ®ç›‘æ§",
            "å¦‚ä½•ç›‘æ§æ•°æ®ï¼Ÿ",
            "æ•°æ®ç›‘æ§æ–¹æ³•",
            "æ•°æ®ç›‘æ§æ€ä¹ˆåšï¼Ÿ",
            "ç›‘æ§æ•°æ®æµç¨‹",
            "æ•°æ®å®‰å…¨",
            "å¦‚ä½•ä¿è¯æ•°æ®å®‰å…¨ï¼Ÿ",
            "æ•°æ®å®‰å…¨æªæ–½",
            "æ•°æ®å®‰å…¨æ€ä¹ˆåšï¼Ÿ",
            "æ•°æ®åŠ å¯†æ–¹æ³•",
            "æ•°æ®é¢„å¤„ç†",
            "æ•°æ®è½¬æ¢",
            "æ•°æ®æ ‡å‡†åŒ–",
            "æ•°æ®å¤‡ä»½",
            "æƒé™ç®¡ç†",
            "ä»»åŠ¡è°ƒåº¦",
            "å¼‚å¸¸å¤„ç†",
            "æ•°æ®åº“è¿æ¥",
            "æ•°æ®å¤„ç†æµç¨‹",
            "ETLæµç¨‹",
            "æ•°æ®ä»“åº“",
            "æ•°æ®å¹³å°æ¶æ„",
            "æ•°æ®æ²»ç†",
            "æ•°æ®è¡€ç¼˜",
            "å…ƒæ•°æ®ç®¡ç†"
        ]

        # é€šç”¨å¯¹è¯ï¼ˆç±»åˆ«1ï¼‰
        general_chat_questions = [
            "ä½ å¥½",
            "æ‚¨å¥½",
            "hi",
            "hello",
            "æ—©ä¸Šå¥½",
            "ä¸‹åˆå¥½",
            "æ™šä¸Šå¥½",
            "è°¢è°¢",
            "è°¢è°¢ä½ ",
            "æ„Ÿè°¢",
            "ä¸å®¢æ°”",
            "å†è§",
            "æ‹œæ‹œ",
            "bye",
            "å¥½çš„",
            "çŸ¥é“äº†",
            "æ˜ç™½äº†",
            "æ”¶åˆ°",
            "OK",
            "å¯ä»¥",
            "æ²¡é—®é¢˜",
            "å¸®å¿™",
            "è¯·é—®",
            "èƒ½å¦",
            "éº»çƒ¦",
            "æ‰“æ‰°äº†",
            "ä¸å¥½æ„æ€",
            "å¯¹ä¸èµ·",
            "æŠ±æ­‰"
        ]

        # æ— å…³é—®é¢˜ï¼ˆç±»åˆ«2ï¼‰
        irrelevant_questions = [
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
            "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½åƒçš„ï¼Ÿ",
            "å¦‚ä½•å­¦ä¹ è‹±è¯­ï¼Ÿ",
            "Pythonæ€ä¹ˆå®‰è£…ï¼Ÿ",
            "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "è‚¡ç¥¨ä»Šå¤©æ¶¨äº†å—ï¼Ÿ",
            "ç”µå½±æ¨èä¸€ä¸‹",
            "éŸ³ä¹å¥½å¬å—ï¼Ÿ",
            "æ¸¸æˆæ”»ç•¥",
            "æ—…æ¸¸æ™¯ç‚¹æ¨è",
            "ç¾é£Ÿåˆ¶ä½œæ–¹æ³•",
            "å¥èº«è®¡åˆ’",
            "å‡è‚¥æ–¹æ³•",
            "æŠ¤è‚¤å“æ¨è",
            "æ±½è½¦ä¿å…»",
            "æˆ¿ä»·èµ°åŠ¿",
            "æ•™è‚²æ”¿ç­–",
            "åŒ»ç–—ä¿é™©",
            "æ³•å¾‹å’¨è¯¢",
            "å¿ƒç†å¥åº·",
            "what is your name?",
            "how are you?",
            "where are you from?",
            "tell me a joke",
            "sing a song",
            "write a poem",
            "calculate 1+1",
            "what time is it?",
            "random text here",
            "asdfghjkl",
            "123456789",
            "æµ‹è¯•æ–‡æœ¬",
            "éšæœºè¾“å…¥",
            "æ— æ„ä¹‰å†…å®¹",
            "ä¹±ä¸ƒå…«ç³Ÿ",
            "èƒ¡è¨€ä¹±è¯­",
            "ä¸çŸ¥æ‰€äº‘",
            "è«åå…¶å¦™",
            "å¥‡æ€ªé—®é¢˜",
            "æ— å…³å†…å®¹"
        ]

        # ç»„åˆæ•°æ®
        texts = data_platform_questions + general_chat_questions + irrelevant_questions
        labels = ([0] * len(data_platform_questions) +
                  [1] * len(general_chat_questions) +
                  [2] * len(irrelevant_questions))

        logger.info(f"å‡†å¤‡äº† {len(texts)} ä¸ªè®­ç»ƒæ ·æœ¬")
        logger.info(f"æ•°æ®å¹³å°ç›¸å…³: {len(data_platform_questions)} ä¸ª")
        logger.info(f"é€šç”¨å¯¹è¯: {len(general_chat_questions)} ä¸ª")
        logger.info(f"æ— å…³é—®é¢˜: {len(irrelevant_questions)} ä¸ª")

        return texts, labels

    def create_dataloader(self, texts, labels, batch_size=8, shuffle=True):
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        dataset = QuestionDataset(texts, labels, self.tokenizer)
        dataloader = DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=0  # Windowsç³»ç»Ÿå»ºè®®è®¾ä¸º0
        )
        return dataloader

    def train(self, epochs=5, batch_size=8, learning_rate=1e-4):
        """è®­ç»ƒLoRAåˆ†ç±»å™¨"""
        logger.info("å¼€å§‹è®­ç»ƒLoRAåˆ†ç±»å™¨...")

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        texts, labels = self.prepare_training_data()

        # åˆ›å»ºæ•°æ®åŠ è½½å™¨
        train_dataloader = self.create_dataloader(texts, labels, batch_size, shuffle=True)

        # è®¾ç½®ä¼˜åŒ–å™¨ - åªä¼˜åŒ–å¯è®­ç»ƒå‚æ•°
        trainable_params = [p for p in self.model.parameters() if p.requires_grad]
        optimizer = AdamW(trainable_params, lr=learning_rate, weight_decay=0.01)

        # è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps = len(train_dataloader) * epochs
        scheduler = get_linear_schedule_with_warmup(
            optimizer,
            num_warmup_steps=int(0.1 * total_steps),  # 10%çš„æ­¥æ•°ç”¨äºwarmup
            num_training_steps=total_steps
        )

        self.model.train()
        best_loss = float('inf')

        for epoch in range(epochs):
            total_loss = 0
            progress_bar = tqdm(train_dataloader, desc=f'Epoch {epoch + 1}/{epochs}')

            for batch in progress_bar:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                loss = outputs.loss

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(trainable_params, max_norm=1.0)

                optimizer.step()
                scheduler.step()

                total_loss += loss.item()
                progress_bar.set_postfix({'loss': loss.item()})

            avg_loss = total_loss / len(train_dataloader)
            logger.info(f'Epoch {epoch + 1}/{epochs}, Average Loss: {avg_loss:.4f}')

            # ä¿å­˜æœ€ä½³æ¨¡å‹
            if avg_loss < best_loss:
                best_loss = avg_loss
                self.save_lora_weights('best_model')

        logger.info("LoRAè®­ç»ƒå®Œæˆï¼")

    def evaluate(self, texts, labels):
        """è¯„ä¼°LoRAåˆ†ç±»å™¨"""
        logger.info("å¼€å§‹è¯„ä¼°LoRAåˆ†ç±»å™¨...")

        eval_dataloader = self.create_dataloader(texts, labels, batch_size=8, shuffle=False)

        self.model.eval()
        predictions = []
        true_labels = []

        with torch.no_grad():
            for batch in tqdm(eval_dataloader, desc='Evaluating'):
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['label'].to(self.device)

                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask
                )

                # è·å–é¢„æµ‹ç»“æœ
                logits = outputs.logits
                predicted_labels = torch.argmax(logits, dim=1)

                predictions.extend(predicted_labels.cpu().numpy())
                true_labels.extend(labels.cpu().numpy())

        # è®¡ç®—å‡†ç¡®ç‡
        accuracy = accuracy_score(true_labels, predictions)
        logger.info(f"å‡†ç¡®ç‡: {accuracy:.4f}")

        # æ‰“å°åˆ†ç±»æŠ¥å‘Š
        target_names = ['æ•°æ®å¹³å°ç›¸å…³', 'é€šç”¨å¯¹è¯', 'æ— å…³é—®é¢˜']
        report = classification_report(true_labels, predictions, target_names=target_names)
        logger.info(f"åˆ†ç±»æŠ¥å‘Š:\n{report}")

        return accuracy

    def save_lora_weights(self, model_name='lora_question_classifier'):
        """ä¿å­˜LoRAæƒé‡"""
        save_path = os.path.join(self.save_dir, model_name)
        os.makedirs(save_path, exist_ok=True)

        # ä¿å­˜LoRAæƒé‡
        lora_state_dict = {}
        for name, module in self.model.named_modules():
            if isinstance(module, LoRALayer):
                lora_state_dict[f"{name}.lora_A.weight"] = module.lora_A.weight
                lora_state_dict[f"{name}.lora_B.weight"] = module.lora_B.weight

        # ä¿å­˜åˆ†ç±»å¤´æƒé‡
        classifier_state_dict = self.model.classifier.state_dict()

        # åˆå¹¶ä¿å­˜
        torch.save({
            'lora_weights': lora_state_dict,
            'classifier_weights': classifier_state_dict,
            'lora_config': {
                'rank': self.lora_rank,
                'alpha': self.lora_alpha,
                'dropout': self.lora_dropout
            }
        }, os.path.join(save_path, 'lora_weights.pth'))

        # ä¿å­˜tokenizer
        self.tokenizer.save_pretrained(save_path)

        # ä¿å­˜æ ‡ç­¾æ˜ å°„
        label_map_path = os.path.join(save_path, 'label_map.json')
        with open(label_map_path, 'w', encoding='utf-8') as f:
            json.dump(self.label_map, f, indent=2, ensure_ascii=False)

        # ä¿å­˜é…ç½®ä¿¡æ¯
        config_path = os.path.join(save_path, 'lora_config.json')
        config = {
            'base_model_path': self.model_path,
            'lora_rank': self.lora_rank,
            'lora_alpha': self.lora_alpha,
            'lora_dropout': self.lora_dropout,
            'num_labels': 3,
            'label_map': self.label_map
        }
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)

        logger.info(f"LoRAæƒé‡å·²ä¿å­˜åˆ°: {save_path}")
        return save_path

    def load_lora_weights(self, lora_path):
        """åŠ è½½LoRAæƒé‡"""
        weights_path = os.path.join(lora_path, 'lora_weights.pth')

        if not os.path.exists(weights_path):
            raise FileNotFoundError(f"LoRAæƒé‡æ–‡ä»¶ä¸å­˜åœ¨: {weights_path}")

        checkpoint = torch.load(weights_path, map_location=self.device)

        # åŠ è½½LoRAæƒé‡
        lora_weights = checkpoint['lora_weights']
        for name, module in self.model.named_modules():
            if isinstance(module, LoRALayer):
                if f"{name}.lora_A.weight" in lora_weights:
                    module.lora_A.weight.data = lora_weights[f"{name}.lora_A.weight"]
                if f"{name}.lora_B.weight" in lora_weights:
                    module.lora_B.weight.data = lora_weights[f"{name}.lora_B.weight"]

        # åŠ è½½åˆ†ç±»å¤´æƒé‡
        if 'classifier_weights' in checkpoint:
            self.model.classifier.load_state_dict(checkpoint['classifier_weights'])

        logger.info(f"LoRAæƒé‡å·²ä» {lora_path} åŠ è½½")

    def predict(self, text):
        """ä½¿ç”¨LoRAæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
        self.model.eval()

        # ç¼–ç è¾“å…¥æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=128,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)

        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=1)
            predicted_class = torch.argmax(logits, dim=1).item()
            confidence = probabilities[0][predicted_class].item()

        return {
            'category': self.label_map[predicted_class],
            'confidence': confidence,
            'probabilities': probabilities[0].cpu().numpy().tolist()
        }


class LoRAQuestionClassifier:
    """LoRAé—®é¢˜åˆ†ç±»å™¨æ¨ç†ç±»"""

    def __init__(self, base_model_path, lora_path):
        self.base_model_path = base_model_path
        self.lora_path = lora_path
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # åŠ è½½é…ç½®
        config_path = os.path.join(lora_path, 'lora_config.json')
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)

        # åŠ è½½æ ‡ç­¾æ˜ å°„
        label_map_path = os.path.join(lora_path, 'label_map.json')
        with open(label_map_path, 'r', encoding='utf-8') as f:
            self.label_map = json.load(f)
            # è½¬æ¢é”®ä¸ºæ•´æ•°
            self.label_map = {int(k): v for k, v in self.label_map.items()}

        # åˆå§‹åŒ–æ¨¡å‹
        self.tokenizer = BertTokenizer.from_pretrained(lora_path)
        self.model = BertForSequenceClassification.from_pretrained(
            base_model_path,
            num_labels=self.config['num_labels']
        )

        # åº”ç”¨LoRA
        self._apply_lora()

        # åŠ è½½LoRAæƒé‡
        self._load_lora_weights()

        self.model.to(self.device)
        self.model.eval()

        logger.info(f"LoRAåˆ†ç±»å™¨æ¨ç†æ¨¡å‹å·²åŠ è½½åˆ°è®¾å¤‡: {self.device}")

    def _apply_lora(self):
        """åº”ç”¨LoRAåˆ°æ¨¡å‹"""
        lora_rank = self.config['lora_rank']
        lora_alpha = self.config['lora_alpha']
        lora_dropout = self.config['lora_dropout']

        # åº”ç”¨LoRAåˆ°BERTçš„attentionå±‚
        for layer in self.model.bert.encoder.layer:
            # Query, Key, ValueæŠ•å½±å±‚
            layer.attention.self.query = LoRALayer(
                layer.attention.self.query,
                rank=lora_rank,
                alpha=lora_alpha,
                dropout=lora_dropout
            )
            layer.attention.self.key = LoRALayer(
                layer.attention.self.key,
                rank=lora_rank,
                alpha=lora_alpha,
                dropout=lora_dropout
            )
            layer.attention.self.value = LoRALayer(
                layer.attention.self.value,
                rank=lora_rank,
                alpha=lora_alpha,
                dropout=lora_dropout
            )

            # å‰é¦ˆç½‘ç»œçš„ç¬¬ä¸€å±‚
            layer.intermediate.dense = LoRALayer(
                layer.intermediate.dense,
                rank=lora_rank,
                alpha=lora_alpha,
                dropout=lora_dropout
            )

    def _load_lora_weights(self):
        """åŠ è½½LoRAæƒé‡"""
        weights_path = os.path.join(self.lora_path, 'lora_weights.pth')
        checkpoint = torch.load(weights_path, map_location=self.device)

        # åŠ è½½LoRAæƒé‡
        lora_weights = checkpoint['lora_weights']
        for name, module in self.model.named_modules():
            if isinstance(module, LoRALayer):
                if f"{name}.lora_A.weight" in lora_weights:
                    module.lora_A.weight.data = lora_weights[f"{name}.lora_A.weight"]
                if f"{name}.lora_B.weight" in lora_weights:
                    module.lora_B.weight.data = lora_weights[f"{name}.lora_B.weight"]

        # åŠ è½½åˆ†ç±»å¤´æƒé‡
        if 'classifier_weights' in checkpoint:
            self.model.classifier.load_state_dict(checkpoint['classifier_weights'])

    def classify_question(self, text):
        """å¯¹é—®é¢˜è¿›è¡Œåˆ†ç±»"""
        # ç¼–ç è¾“å…¥æ–‡æœ¬
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=128,
            return_tensors='pt'
        )

        input_ids = encoding['input_ids'].to(self.device)
        attention_mask = encoding['attention_mask'].to(self.device)

        with torch.no_grad():
            outputs = self.model(input_ids=input_ids, attention_mask=attention_mask)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=1)
            predicted_class = torch.argmax(logits, dim=1).item()
            confidence = probabilities[0][predicted_class].item()

        return {
            'category': self.label_map[predicted_class],
            'confidence': confidence,
            'probabilities': probabilities[0].cpu().numpy().tolist(),
            'method': 'lora_bert'
        }


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # æ¨¡å‹è·¯å¾„ - ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„è·¯å¾„
    bert_model_path = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea"

    print("=" * 60)
    print("ğŸš€ LoRAé—®é¢˜åˆ†ç±»å™¨è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    print("åŸºäºBERT-base-chineseæ¨¡å‹ï¼Œä½¿ç”¨LoRAæŠ€æœ¯è¿›è¡Œé«˜æ•ˆå¾®è°ƒ")
    print("ä¿ç•™æ¨¡å‹åŸæœ‰èƒ½åŠ›çš„åŒæ—¶å®ç°é—®é¢˜ä¸‰åˆ†ç±»")
    print("=" * 60)

    # åˆ›å»ºLoRAè®­ç»ƒå™¨
    trainer = LoRAQuestionClassifierTrainer(
        model_path=bert_model_path,
        save_dir='E:/project/llm/lora/lora_question_classifier',
        lora_rank=8,  # LoRAç§©ï¼Œæ§åˆ¶å‚æ•°é‡
        lora_alpha=16,  # LoRAç¼©æ”¾å› å­
        lora_dropout=0.1  # LoRA dropoutç‡
    )

    # å¼€å§‹è®­ç»ƒ
    print("\nå¼€å§‹LoRAè®­ç»ƒ...")
    trainer.train(
        epochs=5,  # è®­ç»ƒè½®æ•°
        batch_size=8,  # æ‰¹æ¬¡å¤§å°
        learning_rate=1e-4  # å­¦ä¹ ç‡ï¼Œæ¯”å…¨é‡å¾®è°ƒç¨é«˜
    )

    # è¯„ä¼°æ¨¡å‹
    print("\nå¼€å§‹æ¨¡å‹è¯„ä¼°...")
    texts, labels = trainer.prepare_training_data()
    accuracy = trainer.evaluate(texts, labels)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nä¿å­˜æœ€ç»ˆæ¨¡å‹...")
    save_path = trainer.save_lora_weights('final_model')

    print("\n" + "=" * 60)
    print("âœ… LoRAè®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.4f}")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜è·¯å¾„: {save_path}")
    print("=" * 60)

    # æµ‹è¯•æ¨ç†
    print("\nğŸ§ª æµ‹è¯•LoRAæ¨ç†...")
    try:
        # åˆ›å»ºæ¨ç†å™¨
        classifier = LoRAQuestionClassifier(
            base_model_path=bert_model_path,
            lora_path=os.path.join(save_path)
        )

        # æµ‹è¯•é—®é¢˜
        test_questions = [
            "æ•°æ®æ¸…æ´—æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",  # æ•°æ®å¹³å°ç›¸å…³
            "ä½ å¥½",  # é€šç”¨å¯¹è¯
            "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",  # æ— å…³é—®é¢˜
        ]

        print("\næµ‹è¯•ç»“æœï¼š")
        print("-" * 40)
        for question in test_questions:
            result = classifier.classify_question(question)
            category_cn = {
                "data_platform": "æ•°æ®å¹³å°ç›¸å…³",
                "general_chat": "é€šç”¨å¯¹è¯",
                "irrelevant": "æ— å…³é—®é¢˜"
            }
            print(f"é—®é¢˜: {question}")
            print(f"åˆ†ç±»: {category_cn[result['category']]} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
            print("-" * 40)

    except Exception as e:
        print(f"æ¨ç†æµ‹è¯•å¤±è´¥: {e}")

    return save_path


def test_lora_classifier():
    """æµ‹è¯•å·²è®­ç»ƒçš„LoRAåˆ†ç±»å™¨"""
    bert_model_path = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea"
    lora_path = r"E:\project\llm\lora\lora_question_classifier\final_model"

    if not os.path.exists(lora_path):
        print(f"LoRAæ¨¡å‹ä¸å­˜åœ¨: {lora_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒç¨‹åº")
        return

    print("=" * 60)
    print("ğŸ§ª LoRAé—®é¢˜åˆ†ç±»å™¨æµ‹è¯•")
    print("=" * 60)

    # åˆ›å»ºåˆ†ç±»å™¨
    classifier = LoRAQuestionClassifier(
        base_model_path=bert_model_path,
        lora_path=lora_path
    )

    # æµ‹è¯•é—®é¢˜
    test_questions = [
        # æ•°æ®å¹³å°ç›¸å…³
        "æ•°æ®æ¸…æ´—æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•è¿›è¡Œæ•°æ®å…¥åº“ï¼Ÿ",
        "æ•°æ®è´¨é‡æ£€æŸ¥æ–¹æ³•",
        "æ•°æ®ç›‘æ§æ€ä¹ˆåšï¼Ÿ",
        "æ•°æ®å®‰å…¨æªæ–½",

        # é€šç”¨å¯¹è¯
        "ä½ å¥½",
        "è°¢è°¢",
        "å†è§",
        "ä¸å®¢æ°”",
        "å¥½çš„",

        # æ— å…³é—®é¢˜
        "ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·ï¼Ÿ",
        "åŒ—äº¬æœ‰ä»€ä¹ˆå¥½åƒçš„ï¼Ÿ",
        "å¦‚ä½•å­¦ä¹ è‹±è¯­ï¼Ÿ",
        "what is your name?",
        "asdfghjkl"
    ]

    print("æµ‹è¯•ç»“æœï¼š")
    print("-" * 60)

    category_cn = {
        "data_platform": "æ•°æ®å¹³å°ç›¸å…³",
        "general_chat": "é€šç”¨å¯¹è¯",
        "irrelevant": "æ— å…³é—®é¢˜"
    }

    for question in test_questions:
        result = classifier.classify_question(question)
        print(f"é—®é¢˜: {question}")
        print(f"åˆ†ç±»: {category_cn[result['category']]} (ç½®ä¿¡åº¦: {result['confidence']:.3f})")
        print(f"æ–¹æ³•: {result['method']}")
        print("-" * 40)


def interactive_lora_test():
    """äº¤äº’å¼LoRAåˆ†ç±»å™¨æµ‹è¯•"""
    bert_model_path = r"E:\project\llm\model-data\base-models\models--bert-base-chinese\snapshots\8f23c25b06e129b6c986331a13d8d025a92cf0ea"
    lora_path = r"E:\project\llm\lora\lora_question_classifier\final_model"

    if not os.path.exists(lora_path):
        print(f"LoRAæ¨¡å‹ä¸å­˜åœ¨: {lora_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒç¨‹åº")
        return

    print("=" * 60)
    print("ğŸ¤– äº¤äº’å¼LoRAåˆ†ç±»å™¨æµ‹è¯•")
    print("=" * 60)

    classifier = LoRAQuestionClassifier(
        base_model_path=bert_model_path,
        lora_path=lora_path
    )

    category_cn = {
        "data_platform": "æ•°æ®å¹³å°ç›¸å…³",
        "general_chat": "é€šç”¨å¯¹è¯",
        "irrelevant": "æ— å…³é—®é¢˜"
    }

    print("è¾“å…¥é—®é¢˜è¿›è¡Œåˆ†ç±»æµ‹è¯•ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("-" * 60)

    while True:
        try:
            question = input("è¯·è¾“å…¥é—®é¢˜: ").strip()

            if question.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                break

            if not question:
                continue

            result = classifier.classify_question(question)

            print(f"åˆ†ç±»ç»“æœ: {category_cn[result['category']]}")
            print(f"ç½®ä¿¡åº¦: {result['confidence']:.3f}")
            print(f"åˆ†ç±»æ–¹æ³•: {result['method']}")
            print("-" * 40)

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"å¤„ç†é”™è¯¯: {e}")


if __name__ == "__main__":
    # é»˜è®¤æ˜¾ç¤ºèœå•
    while True:
        print("\n" + "=" * 60)
        print("ğŸ¤– LoRAé—®é¢˜åˆ†ç±»å™¨ç³»ç»Ÿ")
        print("=" * 60)
        print("1. è®­ç»ƒLoRAåˆ†ç±»å™¨")
        print("2. æµ‹è¯•LoRAåˆ†ç±»å™¨")
        print("3. äº¤äº’å¼æµ‹è¯•")
        print("0. é€€å‡º")
        print("=" * 60)

        try:
            choice = input("è¯·é€‰æ‹©åŠŸèƒ½ (0-3): ").strip()

            if choice == "1":
                main()
            elif choice == "2":
                test_lora_classifier()
            elif choice == "3":
                interactive_lora_test()
            elif choice == "0":
                print("æ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼")
                break
            else:
                print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")

        except KeyboardInterrupt:
            print("\næ„Ÿè°¢ä½¿ç”¨ï¼å†è§ï¼")
            break
        except Exception as e:
            print(f"å‘ç”Ÿé”™è¯¯: {e}")
