from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import torch

# æ¨¡å‹æœ¬åœ°è·¯å¾„
model_path = r"E:\project\llm\model-data\base-models\TinyLlama-1.1B-Chat-v1.0"

# é…ç½®é‡åŒ–å‚æ•°ï¼ˆæ˜¾å­˜ä¸è¶³æ—¶å¯ç”¨ï¼Œå¯é€‰ï¼‰
# bnb_config = BitsAndBytesConfig(
#     load_in_4bit=True,  # 4bité‡åŒ–ï¼ˆä¹Ÿå¯è®¾ä¸ºload_in_8bit=Trueï¼‰
#     bnb_4bit_use_double_quant=True,
#     bnb_4bit_quant_type="nf4",
#     bnb_4bit_compute_dtype=torch.bfloat16
# )


def interactive_peft_lora_test():
    print("=" * 60)
    print("ğŸ¤– äº¤äº’å¼PEFT LoRAåˆ†ç±»å™¨æµ‹è¯•")
    print("=" * 60)

    print("è¾“å…¥é—®é¢˜è¿›è¡Œåˆ†ç±»æµ‹è¯•ï¼Œè¾“å…¥ 'quit' é€€å‡º")
    print("-" * 60)

    # åŠ è½½tokenizerå’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        # quantization_config=bnb_config,  # è‹¥ä¸é‡åŒ–å¯åˆ é™¤æ­¤è¡Œ
        device_map="auto",  # è‡ªåŠ¨åˆ†é…è®¾å¤‡ï¼ˆä¼˜å…ˆGPUï¼‰
        trust_remote_code=True
    )

    while True:
        try:
            question = input("è¯·è¾“å…¥é—®é¢˜: ").strip()

            if question.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                break

            if not question:
                continue

            # æ„é€ TinyLlamaè¦æ±‚çš„å¯¹è¯æ ¼å¼ï¼ˆå‚è€ƒå®˜æ–¹è¯´æ˜ï¼‰
            formatted_prompt = f"<|user|>\n{question}\n<|assistant|>\n"

            # ç¼–ç è¾“å…¥
            inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)

            # ç”Ÿæˆå›å¤ï¼ˆå¯è°ƒæ•´å‚æ•°æ§åˆ¶ç”Ÿæˆæ•ˆæœï¼‰
            outputs = model.generate(
                **inputs,
                max_new_tokens=100,  # æœ€å¤§ç”Ÿæˆé•¿åº¦
                temperature=0.7,  # éšæœºæ€§ï¼ˆ0-1ï¼Œå€¼è¶Šå°è¶Šç¡®å®šï¼‰
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id
            )

            # è§£ç å¹¶æ‰“å°ç»“æœ
            response = tokenizer.decode(outputs[0], skip_special_tokens=True)
            print("æ¨¡å‹å›å¤ï¼š\n", response)

        except KeyboardInterrupt:
            break
        except Exception as e:
            print(f"å¤„ç†é”™è¯¯: {e}")


if __name__ == "__main__":
    interactive_peft_lora_test()
