import os
import sys
import warnings
import subprocess

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# æ¨¡å‹æœ¬åœ°è·¯å¾„
model_path = r"E:\project\llm\model-data\base-models\chatglm3-6b"

def check_and_fix_environment():
    """æ£€æŸ¥å¹¶ä¿®å¤ç¯å¢ƒé—®é¢˜"""
    print("=" * 60)
    print("ğŸ”§ ChatGLM3-6B ç¯å¢ƒä¿®å¤å·¥å…·")
    print("=" * 60)
    
    print("ğŸ” æ£€æŸ¥å½“å‰ç¯å¢ƒ...")
    
    # æ£€æŸ¥transformersç‰ˆæœ¬
    try:
        import transformers
        print(f"  transformersç‰ˆæœ¬: {transformers.__version__}")
        
        # æ£€æŸ¥ç‰ˆæœ¬å…¼å®¹æ€§
        version = transformers.__version__
        major, minor = map(int, version.split('.')[:2])
        
        if major > 4 or (major == 4 and minor > 35):
            print("  âš ï¸  transformersç‰ˆæœ¬å¯èƒ½è¿‡æ–°ï¼Œå»ºè®®é™çº§")
            print("  å»ºè®®ç‰ˆæœ¬: 4.30.0 - 4.35.0")
            
            user_input = input("æ˜¯å¦è‡ªåŠ¨é™çº§transformers? (y/n): ").strip().lower()
            if user_input in ['y', 'yes', 'æ˜¯']:
                print("æ­£åœ¨é™çº§transformers...")
                try:
                    subprocess.check_call([
                        sys.executable, "-m", "pip", "install", 
                        "transformers==4.30.0", "--force-reinstall"
                    ])
                    print("âœ… transformersé™çº§æˆåŠŸï¼Œè¯·é‡å¯Pythonç¯å¢ƒ")
                    return False
                except Exception as e:
                    print(f"âŒ é™çº§å¤±è´¥: {e}")
        
    except ImportError:
        print("âŒ transformersæœªå®‰è£…")
        return False
    
    # æ£€æŸ¥torchç‰ˆæœ¬
    try:
        import torch
        print(f"  torchç‰ˆæœ¬: {torch.__version__}")
    except ImportError:
        print("âŒ torchæœªå®‰è£…")
        return False
    
    return True

def test_with_different_methods():
    """ä½¿ç”¨ä¸åŒæ–¹æ³•æµ‹è¯•æ¨¡å‹åŠ è½½"""
    if not check_and_fix_environment():
        return
    
    print("\nğŸ§ª å°è¯•ä¸åŒçš„åŠ è½½æ–¹æ³•...")
    
    # æ–¹æ³•1: ä½¿ç”¨safetensors (å¦‚æœå­˜åœ¨)
    print("\næ–¹æ³•1: æ£€æŸ¥safetensorsæ ¼å¼...")
    safetensors_files = [f for f in os.listdir(model_path) if f.endswith('.safetensors')]
    if safetensors_files:
        print(f"  å‘ç°safetensorsæ–‡ä»¶: {len(safetensors_files)}ä¸ª")
        try:
            from transformers import AutoTokenizer, AutoModel
            print("  å°è¯•ä½¿ç”¨safetensorsåŠ è½½...")
            model = AutoModel.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=True,
                torch_dtype="auto",
                use_safetensors=True
            )
            print("  âœ… safetensorsåŠ è½½æˆåŠŸ!")
            return model
        except Exception as e:
            print(f"  âŒ safetensorsåŠ è½½å¤±è´¥: {str(e)[:100]}...")
    else:
        print("  æœªå‘ç°safetensorsæ–‡ä»¶")
    
    # æ–¹æ³•2: åˆ†ç‰‡åŠ è½½
    print("\næ–¹æ³•2: å°è¯•åˆ†ç‰‡åŠ è½½...")
    try:
        from transformers import AutoTokenizer, AutoModel
        import torch
        
        print("  è®¾ç½®å†…å­˜é™åˆ¶...")
        os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "max_split_size_mb:512"
        
        model = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True,
            torch_dtype=torch.float32,
            device_map="cpu",
            max_memory={0: "8GB", "cpu": "16GB"},
            offload_folder="./temp_offload",
            low_cpu_mem_usage=True
        )
        print("  âœ… åˆ†ç‰‡åŠ è½½æˆåŠŸ!")
        return model
    except Exception as e:
        print(f"  âŒ åˆ†ç‰‡åŠ è½½å¤±è´¥: {str(e)[:100]}...")
    
    # æ–¹æ³•3: æ‰‹åŠ¨åŠ è½½
    print("\næ–¹æ³•3: å°è¯•æ‰‹åŠ¨åˆ†æ­¥åŠ è½½...")
    try:
        from transformers import AutoConfig, AutoTokenizer
        import torch
        
        # å…ˆåŠ è½½é…ç½®
        config = AutoConfig.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True
        )
        print("  âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
        # å°è¯•ä½¿ç”¨æ›´ä½çº§çš„API
        from transformers.models.auto.modeling_auto import MODEL_FOR_CAUSAL_LM_MAPPING
        print("  å°è¯•ä½¿ç”¨ä½çº§API...")
        
        # è¿™é‡Œå¯èƒ½éœ€è¦æ›´å…·ä½“çš„å®ç°
        print("  âŒ æ‰‹åŠ¨åŠ è½½éœ€è¦æ›´å¤šå®ç°")
        
    except Exception as e:
        print(f"  âŒ æ‰‹åŠ¨åŠ è½½å¤±è´¥: {str(e)[:100]}...")
    
    # æ–¹æ³•4: ä½¿ç”¨ä¸åŒçš„åº“
    print("\næ–¹æ³•4: å»ºè®®ä½¿ç”¨å…¶ä»–æ–¹æ¡ˆ...")
    print("  ğŸ’¡ å¯èƒ½çš„æ›¿ä»£æ–¹æ¡ˆ:")
    print("  1. ä½¿ç”¨ChatGLMå®˜æ–¹ä»£ç åº“")
    print("  2. ä½¿ç”¨vLLMæˆ–FastChatç­‰æ¨ç†æ¡†æ¶")
    print("  3. ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬çš„æ¨¡å‹")
    print("  4. ä½¿ç”¨Dockerå®¹å™¨ç¯å¢ƒ")
    
    return None

def create_alternative_script():
    """åˆ›å»ºæ›¿ä»£è„šæœ¬"""
    print("\nğŸ“ åˆ›å»ºæ›¿ä»£è§£å†³æ–¹æ¡ˆ...")
    
    alternative_code = '''
# ChatGLM3-6B æ›¿ä»£æ–¹æ¡ˆ
# å¦‚æœtransformersåŠ è½½å¤±è´¥ï¼Œå¯ä»¥å°è¯•ä»¥ä¸‹æ–¹æ³•ï¼š

# æ–¹æ¡ˆ1: ä½¿ç”¨å®˜æ–¹ä»£ç 
# git clone https://github.com/THUDM/ChatGLM3
# ç„¶åä½¿ç”¨å®˜æ–¹çš„åŠ è½½æ–¹å¼

# æ–¹æ¡ˆ2: ä½¿ç”¨é‡åŒ–æ¨¡å‹
# ä¸‹è½½int4æˆ–int8é‡åŒ–ç‰ˆæœ¬ï¼Œå†…å­˜å ç”¨æ›´å°

# æ–¹æ¡ˆ3: ä½¿ç”¨APIæ–¹å¼
# è€ƒè™‘ä½¿ç”¨åœ¨çº¿APIæˆ–æœ¬åœ°éƒ¨ç½²çš„APIæœåŠ¡

# æ–¹æ¡ˆ4: ç¯å¢ƒéš”ç¦»
# åˆ›å»ºæ–°çš„condaç¯å¢ƒï¼Œä½¿ç”¨ç‰¹å®šç‰ˆæœ¬çš„ä¾èµ–
# conda create -n chatglm python=3.9
# conda activate chatglm
# pip install transformers==4.30.0 torch==1.13.1

print("è¯·å‚è€ƒæ³¨é‡Šä¸­çš„æ›¿ä»£æ–¹æ¡ˆ")
'''
    
    with open("src/chatglm3_6b/alternative_solutions.py", "w", encoding="utf-8") as f:
        f.write(alternative_code)
    
    print("âœ… å·²åˆ›å»ºæ›¿ä»£æ–¹æ¡ˆæ–‡ä»¶: src/chatglm3_6b/alternative_solutions.py")

def main():
    """ä¸»å‡½æ•°"""
    model = test_with_different_methods()
    
    if model is None:
        create_alternative_script()
        print("\n" + "=" * 60)
        print("âŒ æ‰€æœ‰åŠ è½½æ–¹æ³•éƒ½å¤±è´¥äº†")
        print("=" * 60)
        print("ğŸ”§ å»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. é™çº§transformers: pip install transformers==4.30.0")
        print("2. é‡æ–°ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
        print("3. ä½¿ç”¨å®˜æ–¹ChatGLM3ä»£ç åº“")
        print("4. è€ƒè™‘ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬æ¨¡å‹")
        print("5. åˆ›å»ºæ–°çš„condaç¯å¢ƒæµ‹è¯•")
    else:
        print("\nâœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å¯ä»¥è¿›è¡Œæµ‹è¯•")
        
        # ç®€å•æµ‹è¯•
        try:
            from transformers import AutoTokenizer
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=True
            )
            
            test_question = "ä½ å¥½"
            response, _ = model.chat(tokenizer, test_question, history=[])
            print(f"æµ‹è¯•å¯¹è¯: {test_question} -> {response}")
            
        except Exception as e:
            print(f"å¯¹è¯æµ‹è¯•å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()
