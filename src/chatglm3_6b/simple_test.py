import os
import torch
import warnings
from transformers import AutoTokenizer, AutoModel

# å¿½ç•¥è­¦å‘Š
warnings.filterwarnings("ignore")

# æ¨¡å‹æœ¬åœ°è·¯å¾„
model_path = r"E:\project\llm\model-data\base-models\chatglm3-6b"

def test_chatglm3_fixed():
    """
    ä¿®å¤ç‰ˆChatGLM3-6Bæµ‹è¯•
    ä¸“é—¨è§£å†³padding_sideå…¼å®¹æ€§é—®é¢˜
    """
    print("=" * 60)
    print("ğŸ¤– ChatGLM3-6B ä¿®å¤ç‰ˆæµ‹è¯•")
    print("=" * 60)
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
        return
    
    try:
        print("ğŸ”„ åŠ è½½tokenizer...")
        # ä½¿ç”¨æœ€åŸºæœ¬çš„tokenizeråŠ è½½æ–¹å¼
        tokenizer = AutoTokenizer.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True
        )
        print("âœ… tokenizeråŠ è½½æˆåŠŸ")
        
        print("\nğŸ”„ åŠ è½½æ¨¡å‹...")
        print("(ä½¿ç”¨CPUæ¨¡å¼ï¼Œè¯·è€å¿ƒç­‰å¾…...)")
        
        # ä½¿ç”¨æœ€ç¨³å®šçš„æ¨¡å‹åŠ è½½æ–¹å¼
        model = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True,
            torch_dtype=torch.float32,
            device_map=None,
            low_cpu_mem_usage=True
        ).to('cpu').eval()
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        print("\n" + "=" * 60)
        print("å¼€å§‹å¯¹è¯æµ‹è¯•")
        print("=" * 60)
        
        # é¢„è®¾æµ‹è¯•é—®é¢˜
        test_questions = [
            "ä½ å¥½",
            "è¯·ä»‹ç»ä¸€ä¸‹è‡ªå·±",
            "1+1ç­‰äºå‡ ï¼Ÿ"
        ]
        
        for i, question in enumerate(test_questions, 1):
            print(f"\nğŸ“ æµ‹è¯• {i}: {question}")
            try:
                # ä½¿ç”¨æœ€ç®€å•çš„chatè°ƒç”¨ï¼Œé¿å…å¤æ‚å‚æ•°
                response, _ = model.chat(tokenizer, question, history=[])
                print(f"ğŸ¤– å›å¤: {response}")
                print("-" * 40)
            except Exception as e:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {str(e)}")
                # å°è¯•æ›´åŸºç¡€çš„æ–¹å¼
                try:
                    inputs = tokenizer(question, return_tensors="pt")
                    print("âš ï¸  ä½¿ç”¨åŸºç¡€ç”Ÿæˆæ–¹å¼...")
                    with torch.no_grad():
                        outputs = model.generate(**inputs, max_length=100, do_sample=False)
                    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                    print(f"ğŸ¤– å›å¤: {response}")
                except Exception as e2:
                    print(f"âŒ åŸºç¡€ç”Ÿæˆä¹Ÿå¤±è´¥: {str(e2)}")
        
        # äº¤äº’æ¨¡å¼
        print(f"\n{'='*60}")
        print("ğŸ¯ äº¤äº’æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
        print("="*60)
        
        while True:
            try:
                question = input("\nè¯·è¾“å…¥é—®é¢˜: ").strip()
                
                if question.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
                    break
                
                if not question:
                    continue
                
                print("ğŸ¤” æ€è€ƒä¸­...")
                
                # ä¼˜å…ˆä½¿ç”¨chatæ–¹æ³•
                try:
                    response, _ = model.chat(tokenizer, question, history=[])
                    print(f"ğŸ¤– å›å¤: {response}")
                except:
                    # å¤‡ç”¨æ–¹æ¡ˆï¼šç›´æ¥ç”Ÿæˆ
                    try:
                        inputs = tokenizer(question, return_tensors="pt")
                        with torch.no_grad():
                            outputs = model.generate(
                                **inputs,
                                max_length=200,
                                do_sample=True,
                                temperature=0.7,
                                pad_token_id=tokenizer.eos_token_id
                            )
                        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
                        # ç§»é™¤è¾“å…¥éƒ¨åˆ†ï¼Œåªä¿ç•™å›å¤
                        if question in response:
                            response = response.replace(question, "").strip()
                        print(f"ğŸ¤– å›å¤: {response}")
                    except Exception as e:
                        print(f"âŒ ç”Ÿæˆå¤±è´¥: {str(e)}")
                
            except KeyboardInterrupt:
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {str(e)}")
        
        print("\nğŸ‘‹ æµ‹è¯•å®Œæˆ")
        
    except Exception as e:
        print(f"âŒ ç¨‹åºå¤±è´¥: {str(e)}")
        print("\nğŸ’¡ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥transformersç‰ˆæœ¬: pip install transformers==4.30.0")
        print("2. æ£€æŸ¥torchç‰ˆæœ¬å…¼å®¹æ€§")
        print("3. å°è¯•é‡æ–°å®‰è£…ä¾èµ–")

if __name__ == "__main__":
    test_chatglm3_fixed()
