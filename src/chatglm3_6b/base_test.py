import os
import torch
import gc
import warnings
import sys
import psutil
from transformers import AutoTokenizer, AutoModel

# å¿½ç•¥ä¸€äº›è­¦å‘Š
warnings.filterwarnings("ignore")

# æ¨¡å‹æœ¬åœ°è·¯å¾„ - è¯·æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹
model_path = r"E:\project\llm\model-data\base-models\chatglm3-6b"

def check_system_info():
    """æ£€æŸ¥ç³»ç»Ÿä¿¡æ¯"""
    print("ğŸ” ç³»ç»Ÿè¯Šæ–­ä¿¡æ¯:")
    print(f"  Pythonç‰ˆæœ¬: {sys.version}")
    print(f"  PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # å†…å­˜ä¿¡æ¯
    memory = psutil.virtual_memory()
    print(f"  ç³»ç»Ÿå†…å­˜: {memory.total / (1024**3):.1f}GB")
    print(f"  å¯ç”¨å†…å­˜: {memory.available / (1024**3):.1f}GB")
    print(f"  å†…å­˜ä½¿ç”¨ç‡: {memory.percent}%")
    
    # GPUä¿¡æ¯
    if torch.cuda.is_available():
        print(f"  GPU: {torch.cuda.get_device_name(0)}")
        print(f"  GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.1f}GB")
    else:
        print("  GPU: æœªæ£€æµ‹åˆ°")

def check_model_files():
    """æ£€æŸ¥æ¨¡å‹æ–‡ä»¶å®Œæ•´æ€§"""
    print("\nğŸ“ æ£€æŸ¥æ¨¡å‹æ–‡ä»¶:")
    
    if not os.path.exists(model_path):
        print(f"âŒ æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_path}")
        return False
    
    # æ£€æŸ¥å…³é”®æ–‡ä»¶
    required_files = [
        "config.json",
        "tokenizer.model", 
        "tokenizer_config.json"
    ]
    
    for file in required_files:
        file_path = os.path.join(model_path, file)
        if os.path.exists(file_path):
            size = os.path.getsize(file_path)
            print(f"  âœ… {file}: {size:,} bytes")
        else:
            print(f"  âŒ {file}: æ–‡ä»¶ä¸å­˜åœ¨")
    
    # æ£€æŸ¥æ¨¡å‹æƒé‡æ–‡ä»¶
    print("\n  æ¨¡å‹æƒé‡æ–‡ä»¶:")
    for file in os.listdir(model_path):
        if file.endswith(('.bin', '.safetensors')):
            file_path = os.path.join(model_path, file)
            size = os.path.getsize(file_path)
            print(f"    {file}: {size / (1024**3):.2f}GB")
    
    return True

def test_tokenizer_only():
    """ä»…æµ‹è¯•tokenizeråŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•1: ä»…åŠ è½½tokenizer")
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            local_files_only=True
        )
        print("  âœ… tokenizeråŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•tokenizeråŠŸèƒ½ - é¿å…è§¦å‘padding_sideé”™è¯¯
        test_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼"
        print(f"  æµ‹è¯•ç¼–ç æ–‡æœ¬: {test_text}")
        
        # ç›´æ¥ä½¿ç”¨tokenizerçš„åŸºæœ¬åŠŸèƒ½ï¼Œé¿å…å¯èƒ½æœ‰é—®é¢˜çš„æ–¹æ³•
        try:
            tokens = tokenizer.encode(test_text, add_special_tokens=True)
            print(f"  ç¼–ç ç»“æœ: {tokens}")
            
            decoded = tokenizer.decode(tokens, skip_special_tokens=True)
            print(f"  è§£ç ç»“æœ: {decoded}")
            
            print("  âœ… tokenizeråŠŸèƒ½æµ‹è¯•æˆåŠŸ")
            return True
            
        except Exception as encode_error:
            print(f"  âš ï¸  ç¼–ç æµ‹è¯•å¤±è´¥: {str(encode_error)}")
            print("  ä½†tokenizeråŠ è½½æˆåŠŸï¼Œå¯èƒ½ä»å¯ç”¨äºæ¨¡å‹")
            return True  # tokenizeråŠ è½½æˆåŠŸå°±ç®—é€šè¿‡
        
    except Exception as e:
        print(f"  âŒ tokenizeråŠ è½½å¤±è´¥: {str(e)}")
        return False

def test_model_config():
    """æµ‹è¯•æ¨¡å‹é…ç½®åŠ è½½"""
    print("\nğŸ§ª æµ‹è¯•2: åŠ è½½æ¨¡å‹é…ç½®")
    try:
        from transformers import AutoConfig
        config = AutoConfig.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True
        )
        print("  âœ… é…ç½®åŠ è½½æˆåŠŸ")
        print(f"  æ¨¡å‹ç±»å‹: {config.model_type}")
        print(f"  éšè—å±‚å¤§å°: {config.hidden_size}")
        print(f"  å±‚æ•°: {config.num_layers}")
        return True
        
    except Exception as e:
        print(f"  âŒ é…ç½®åŠ è½½å¤±è´¥: {str(e)}")
        return False

def test_minimal_model_load():
    """æœ€å°åŒ–æ¨¡å‹åŠ è½½æµ‹è¯•"""
    print("\nğŸ§ª æµ‹è¯•3: æœ€å°åŒ–æ¨¡å‹åŠ è½½")
    
    # æ£€æŸ¥å¯ç”¨å†…å­˜
    memory = psutil.virtual_memory()
    if memory.available < 8 * 1024**3:  # å°äº8GB
        print(f"  âš ï¸  å¯ç”¨å†…å­˜ä¸è¶³: {memory.available / (1024**3):.1f}GB < 8GB")
        print("  å»ºè®®å…³é—­å…¶ä»–ç¨‹åºé‡Šæ”¾å†…å­˜")
        return False
    
    try:
        print("  æ­£åœ¨å°è¯•åŠ è½½æ¨¡å‹...")
        print("  (è¿™æ˜¯æœ€å®¹æ˜“å´©æºƒçš„æ­¥éª¤)")
        
        # å°è¯•æœ€ä¿å®ˆçš„åŠ è½½æ–¹å¼
        model = AutoModel.from_pretrained(
            model_path,
            trust_remote_code=True,
            local_files_only=True,
            torch_dtype=torch.float32,
            device_map=None,  # ä¸ä½¿ç”¨device_map
            low_cpu_mem_usage=True,  # é‡æ–°å¯ç”¨ä½å†…å­˜æ¨¡å¼
            offload_folder=None
        )
        
        # æ‰‹åŠ¨ç§»åŠ¨åˆ°CPU
        model = model.to('cpu')
        model.eval()
        
        print("  âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
        return model
        
    except Exception as e:
        print(f"  âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
        print(f"  é”™è¯¯ç±»å‹: {type(e).__name__}")
        return None

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”§ ChatGLM3-6B è¯Šæ–­å·¥å…·")
    print("=" * 60)
    
    # æ­¥éª¤1: ç³»ç»Ÿä¿¡æ¯æ£€æŸ¥
    check_system_info()
    
    # æ­¥éª¤2: æ–‡ä»¶å®Œæ•´æ€§æ£€æŸ¥
    if not check_model_files():
        print("\nâŒ æ¨¡å‹æ–‡ä»¶æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„å’Œæ–‡ä»¶å®Œæ•´æ€§")
        return
    
    # æ­¥éª¤3: tokenizeræµ‹è¯•
    if not test_tokenizer_only():
        print("\nâŒ tokenizeræµ‹è¯•å¤±è´¥")
        return
    
    # æ­¥éª¤4: é…ç½®æµ‹è¯•
    if not test_model_config():
        print("\nâŒ æ¨¡å‹é…ç½®æµ‹è¯•å¤±è´¥")
        return
    
    # æ­¥éª¤5: æ¨¡å‹åŠ è½½æµ‹è¯•
    print("\n" + "=" * 60)
    print("âš ï¸  å³å°†è¿›è¡Œæ¨¡å‹åŠ è½½æµ‹è¯•")
    print("è¿™æ˜¯æœ€å¯èƒ½å‡ºç°0xC0000005é”™è¯¯çš„æ­¥éª¤")
    print("=" * 60)
    
    user_input = input("æ˜¯å¦ç»§ç»­æ¨¡å‹åŠ è½½æµ‹è¯•? (y/n): ").strip().lower()
    if user_input not in ['y', 'yes', 'æ˜¯']:
        print("æµ‹è¯•ä¸­æ­¢")
        return
    
    model = test_minimal_model_load()
    if model is None:
        print("\nâŒ æ¨¡å‹åŠ è½½å¤±è´¥")
        print("\nğŸ”§ å»ºè®®çš„è§£å†³æ–¹æ¡ˆ:")
        print("1. æ£€æŸ¥ç³»ç»Ÿå†…å­˜æ˜¯å¦è¶³å¤Ÿï¼ˆå»ºè®®16GB+ï¼‰")
        print("2. å°è¯•é‡æ–°ä¸‹è½½æ¨¡å‹æ–‡ä»¶")
        print("3. æ£€æŸ¥transformersåº“ç‰ˆæœ¬æ˜¯å¦å…¼å®¹")
        print("4. å°è¯•ä½¿ç”¨é‡åŒ–ç‰ˆæœ¬çš„æ¨¡å‹")
        return
    
    # å¦‚æœæ¨¡å‹åŠ è½½æˆåŠŸï¼Œè¿›è¡Œç®€å•æµ‹è¯•
    print("\nğŸ‰ æ¨¡å‹åŠ è½½æˆåŠŸï¼è¿›è¡ŒåŠŸèƒ½æµ‹è¯•...")
    
    try:
        tokenizer = AutoTokenizer.from_pretrained(
            model_path, 
            trust_remote_code=True,
            local_files_only=True
        )
        
        test_question = "ä½ å¥½"
        print(f"\næµ‹è¯•é—®é¢˜: {test_question}")
        print("å¤„ç†ä¸­...")
        
        response, _ = model.chat(tokenizer, test_question, history=[])
        print(f"æ¨¡å‹å›å¤: {response}")
        
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼æ¨¡å‹å·¥ä½œæ­£å¸¸")
        
    except Exception as e:
        print(f"\nâŒ å¯¹è¯æµ‹è¯•å¤±è´¥: {str(e)}")

if __name__ == "__main__":
    main()
